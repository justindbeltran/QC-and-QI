\documentclass[12pt]{exam}

\usepackage[
  letterpaper,
  margin=0.3in,
  includehead,    % include the header area in the 0.25in margin
  includefoot     % include the footer area in the 0.25in margin
]{geometry}
\usepackage[shortlabels]{enumitem}
\setlist[enumerate,1]{label=(\alph*)} %makes list alphabetic

\usepackage{physics}
\usepackage{amsfonts}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\usepackage{pgfplots} % For sketching graphs
\pgfplotsset{compat=1.18}


\usepackage{caption}

\usepackage[hidelinks]{hyperref}

\makeatletter
  % Save the original \section into \EXAM@orig@section
  \let\EXAM@orig@section\section
  \renewcommand{\section}[1]{
    \refstepcounter{section}
    \EXAM@orig@section*{#1~\thesection}
    
    \addcontentsline{toc}{section}{#1~\thesection}
  }
\makeatother

\usepackage{etoolbox}

\newcounter{exercise}[section]
\renewcommand{\theexercise}{\thesection.\arabic{exercise}}
\newcounter{problem}[section]
\renewcommand{\theproblem}{\thesection.\arabic{problem}}

\makeatletter
  \let\EXAM@orig@subsection\subsection

  %Redefine \subsection itself:
  \renewcommand{\subsection}[1]{
    \ifstrequal{#1}{Exercise}{
      \refstepcounter{exercise}
      \EXAM@orig@subsection*{#1~\theexercise}
      \addcontentsline{toc}{subsection}{#1~\theexercise}
    }{
      \ifstrequal{#1}{Problem}{
        \refstepcounter{problem}
        \EXAM@orig@subsection*{#1~\theproblem}
        \addcontentsline{toc}{subsection}{#1~\theproblem}
      }{
        \refstepcounter{subsection}%
        \EXAM@orig@subsection*{#1~\thesubsection}%
        \addcontentsline{toc}{subsection}{#1~\thesubsection}
      }
    }
  }
\makeatother

\usepackage{hanging}
\usepackage{parskip}



\title{%
  \centering
  Quantum Computation and Quantum Information\\
  Solution Manual\\
  \small
  For Michael A. Nielsen and Isaac L. Chuang’s 10th Anniversary Edition
}

\author{By Justin Beltran}
\date{May 2025}

\begin{document}

\maketitle


\firstpagefooter{}{}{By Justin Beltran}
\runningfooter{}{}{By Justin Beltran}
\printanswers

\newpage


\section{Chapter}
\subsection{Exercise}
\textbf{(Probabilistic classical algorithm)} Suppose that the problem is not to distinguish between the constant and balanced functions \textit{with certainty}, but rather, with some probability of error $\epsilon < 1/2$. What is the performance of the best classical algorithm for this problem?

\begin{solution}
    The best classical algorithm for Deutsch's problem with 100\% success would require $2^{n-1}+1$ queries because Alice may receive $2^{n-1}$ 0's before receiving a 1. The best probabalistic classical algorithm for the same problem with $\epsilon < 1/2$ would be 3 queries. The first query establishes a baseline and the second has probability 1/2 of matching the first but 3 queries would have probability 1/4. Generalizing this for $q$ queries with probability of error $\epsilon$ is $$\epsilon < \frac{1}{2^{q-1}} = 2^{-q+1} \quad \implies \quad \log\frac{1}{\epsilon} + 1 < q \quad \implies \quad O\big(\log\frac{1}{\epsilon}\big)$$
\end{solution}



\subsection{Exercise}
Explain how a device which, upon input of one of two non-orthogonal quantum states $\ket{\psi}$ or $\ket{\varphi}$ correctly identified the state, could be used to build a device which cloned the states $\ket{\psi}$ and $\ket{\varphi}$, in violation of the no-cloning theorem. Conversely, explain how a device for cloning could be used to distinguish non-orthogonal quantum states.

\begin{solution}
    \begin{enumerate}
        \item Given a device that correctly identifies one of two non-orthoganal quantum states, read one of the unknown states in and now there is a perfect classical "image" of the state. Now use that image to prepare the exact same state in the quantum device as many times as desired violating the no-cloning theorem. 

        \item A device that clones quantum states could distinguish non-orthogonal quantum states by setting up $many$ clones then measuring all the clones to reconstruct the quantum state perfectly.
    \end{enumerate}
\end{solution}



\subsection{Problem} 
\textbf{(Feynman - Gates conversation)}
Construct a friendly imaginary discussion of about 2000 words between Bill Gates and Richard Feynman, set in the present, on the future of computation. (\textit{Comment: }You might like to try waiting until you’ve read the rest of the book before attempting this question. See the ‘History and further reading’ below for pointers to one possible answer for this question.)

\begin{solution}
For brevity, this was shortened well below 2000 words:\\
Gates: Isn’t the way you do computing different?\\
Feynman: Yes, rooted in physics and math but computers perform huge calculations and sometimes suggest unconsidered ideas.\\
Gates: How do you view the difference between your work in the war and that now?\\
Feynman: We did what was necessary to win the war. Afterwards, many of us questioned the bomb’s impact. I returned to physics at Caltech, but I learned that even simple machines can yield vital results.\\
Gates: You predicted unseen computers by 2050.\\
Feynman: I studied the physical limits of computation and found no quantum mechanical barrier to miniaturization—only considerations of thermodynamics and reversibility. If you want atomic‐scale machines, you must use quantum mechanics rather than classical physics.\\
Gates: Why derive Shannon’s theorem three different ways?\\
Feynman: Deriving known results independently teaches you new insights. \\
Gates: At Microsoft, we focus on usability and extending human cognition.\\
Feynman: I’m interested. In physics, my Feynman diagrams simplified months of work into an evening. Tools that simplify complexity and reveal new layers are invaluable.\\
Gates: How do you anticipate computing’s future?\\
Feynman: We need computers modeled on natural laws so programs adapt gracefully to changing boundary conditions, reducing unforeseen errors.\\
Gates: Does that mean programmers must be physicists?\\
Feynman: Fundamental physical laws are simple; knowing some physics helps, but it isn’t mandatory.\\
Gates: How would nature-based model software affect testing?\\
Feynman: Testing becomes more straightforward—failures can reveal new phenomena rather than just bugs.\\
Gates: So software can be “quantized”?\\
Feynman: Yes—small, self‐contained modules that solve specific problems.\\
Gates: Good software engineers draw inspiration across disciplines.\\
Feynman: Agreed—like in physics, varied perspectives uncover truths.\\
Gates: Your most interesting work since your Nobel Prize?\\
Feynman: Theory of liquid helium, laws of weak interaction, parton theory. Now I’m just playing with new ideas.\\
Gates: Any disappointments?\\
Feynman: Turbulence and superconductivity remain unsolved.  I failed but tackling them was rewarding.\\
Gates: Complexity’s big question?\\
Feynman: How simple rules give rise to complex behavior from bee swarms to neural networks. It demands interdisciplinary study.\\
Gates: Will you consult for Microsoft?\\
Feynman: That’s the “wackiest idea” I’ve heard! Perhaps. A software revolution is coming; imagination is key.
\end{solution}



\subsection{Problem} What is the most significant discovery yet made in quantum computation and quantum information? Write an essay of about 2000 words for an educated lay audience about the discovery. (\textit{Comment}: As for the previous problem, you might like to try waiting until you’ve read the rest of the book before attempting this question.)

\begin{solution}
Again for brevity, this was shortened well below 2000 words:\\
    The most important discovery that has been made in quantum computation and quantum information I believe is the qubit $\ket{\psi}$. Without high-level quantum computers being widely available, algorithm designers or more generally theorists could not continue to research or develop ideas without the mathematical formalism of the qubit. That is the the notion of a fundamental unit of quantum information is a two-level quantum system, i.e. the qubit, rather than a classical bit or bitstring. Further that this qubit can exist in a coherent superposition $\ket{\psi} = \alpha \ket{0} + \beta\ket{1}$ and again more generally that $\alpha$ and $\beta$ can be any complex coefficient in $\C$ that obey $|\alpha|^2 + |\beta|^2  = 1$. This laid the groundwork for encoding information in quantum amplitudes and opened the door to phenomena such as entanglement or interference which have no classical analogue. These additional processes are where the most significant algorithms like Shor's or Grover's extract most of their power. 
    
    This formalism of the qubit also extends nicely to
    many body systems via the tensor product $\otimes$ where two qubit's, $\ket{\psi}$ and $\ket{\phi}$, combined state can be written $\ket{\psi} \otimes \ket{\phi}$. From here the idea of the 2-Dimensional Hilbert Space $\mathcal{H_2} = \C^2$ arises and by choosing an orthonormal basis: $\{\ket{0}, \ket{1}\}$, a diagonal basis: $\{\ket{+}, \ket{-}\}$ is generated where  $\ket{+} = (\ket{0} + \ket{1})/\sqrt{2}, \quad \ket{-} = (\ket{0} - \ket{1})/\sqrt{2}$. Combining this concept with the previously mentioned tensor product the Hilbert Space grows with the number of these qubits to $\mathcal{H} = (\C^2)^{\otimes n}$ for $n$ qubits. Which provides enough mathematical foundation for the ideas of quantum computation and quantum information to be explored before the arrival of widespread quantum hardware.
\end{solution}


\vspace{12pt}
\section{Chapter}

\subsection{Exercise}
\textbf{(Linear dependence: example)} Show that (1, -1), (1, 2) and (2, 1) are linearly dependent.

\begin{solution}
    They are linearly dependent if a set $a_1,..., a_n \in \C$ with $a_i \neq 0$ for at least one value of $i$ s.t. $a_1\ket{v_1} + a_2\ket{v_2} + a_3\ket{v_3} = 0$ now choose $a_1 = 1, a_2 = 1, a_3 =-1$ and observe that $$\begin{bmatrix}  1 \\ -1 \end{bmatrix} + \begin{bmatrix}  1 \\ 2 \end{bmatrix} - \begin{bmatrix}  2 \\ 1 \end{bmatrix} = 0 $$ Note this is generalizable to any $x \in \C$ where $a_1 = x, a_2 = x, a_3 =-x$.
\end{solution}



\subsection{Exercise} \textbf{(Matrix representations: example)} Suppose V is a vector space with basis vectors $\ket{0}$ and $\ket{1}$, and $A$ is a linear operator from $V$ to $V$ such that $A\ket{0} = \ket{1}$ and  $A\ket{1} = \ket{0}$. Give a matrix representation for $A$, with respect to the input basis $\ket{0}, \ket{1}$, and the output basis $\ket{0}, \ket{1}$. Find input and output bases which give rise to a different matrix representation of $A$.

\begin{solution}
    For vector space $V$ with basis $\ket{0}, \ket{1}$ and $A: V \mapsto V$ as described then 
    $$ A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \implies \quad \begin{bmatrix} a_1 & a_2  \\ a_3 & a_4 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} a_1 \\ a_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$$ which implies $a_1 = 0, a_3 = 1$ then using $A\ket{1} = \ket{0}$ find 
    $$A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \implies \quad \begin{bmatrix} 0 & a_2  \\ 1 & a_4 \end{bmatrix}\begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} a_2 \\ a_4 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ thus $a_2 = 1, a_4 = 0$ so $A = \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}$
\end{solution}



\subsection{Exercise} \textbf{(Matrix representation for operator products)} Suppose $A$ is a linear operator from vector space $V$ to vector space $W$ , and $B$ is a linear operator from vector space $W$ to vector space $X$. Let $\ket{v_i}, \ket{w_j}$, and $ \ket{x_k}$ be bases for the vector spaces $V,W$, and $X$, respectively. Show that the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.

\begin{solution}
    With linear operators $A: V \mapsto W$, $B: W \mapsto X$ with basis $\ket{v_i} \in V, \quad \ket{w_j} \in W$, and $ \ket{x_k}\in X$ start with $BA\ket{v_i}$ then by associativity this equals
    $$B(A\ket{v_i}) = B \sum_j A_{ji}\ket{w_j} =  \sum_j A_{ji}(B\ket{w_j}) = \sum_k \sum_j B_{kj}A_{ji}\ket{x_k}$$ noting that $B_{kj}$ and $A_{ji}$ can be reordered since sums over finite indices are commutative and associative and $(BA)_{ki} $ has entries given by $\sum_j B_{kj} A_{ji}$.
\end{solution}




\subsection{Exercise}
\textbf{(Matrix representation for identity)} Show that the identity operator on a vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases. This matrix is known as the identity matrix.

\begin{solution}
    Let $I: V \mapsto V$ s.t. $I\ket{v} = \ket{v}$, $ \forall\ket{v} \in V$ and the basis for $V$ is $\ket{v_i}$ for $i \in \{1,...,n\}$ then each entry in $I$ is generated by 
    $$ I \ket{v_i} = \sum^n_j I_{ji}\ket{v_j} = \ket{v_i}$$
    thus $I_{ji} = 1 \iff j=i$ and 0 otherwise. Thus the matrix is $$I = \begin{bmatrix}
        1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1
    \end{bmatrix}$$
\end{solution}




\subsection{Exercise} Verify that $(\cdot, \cdot)$ just defined is an inner product on $\C^n$.

\begin{solution}
    Running the checklist for inner products find \begin{enumerate}[1)]
        \item Linearity (in 2nd argument): $$(\ket{y}, \sum_i \lambda_i\ket{z_i}) = \sum_j y_j^*\sum_i \lambda_i z_{ij} = \sum_i \lambda_i\sum_j  y_j^* z_{ij} = \sum_i \lambda_i(\ket{y},\ket{z_i})$$

        \item Conjugate Symmetry:
        $$(\ket{y}, \ket{z}) = \sum_i y_i^*z_i = \sum_i z_iy_i^* = \sum_i (z_i^*y_i)^* = (\ket{z}, \ket{y})^*$$

        \item Positive Definiteness: $$(\ket{y}, \ket{y}) = \sum_i y_i^*y_i =\sum_i |y_i|^2 $$ which is non-negative and note that $\sum|y_i|^2 = 0 \iff \forall i,  y_i = 0$
    \end{enumerate}
    Thus $(\cdot, \cdot)$ as defined is an inner product on $\C^n$.
\end{solution}




\subsection{Exercise} Show that any inner product $(\cdot, \cdot)$ is conjugate-linear in the first argument,

$$\bigg( \sum_i \lambda_i\ket{w_i},\ket{v}\bigg) = \sum_i \lambda_i^*\big(\ket{w_i}, \ket{v}\big)$$

\begin{solution}
    Using the conjugate symmetry property of the  inner product then linearity then conjugate symmetry again  find:
    $$ \bigg( \sum_i \lambda_i\ket{w_i},\ket{v}\bigg)  =  \bigg(\ket{v}, \sum_i \lambda_i\ket{w_i}\bigg)^*  = \sum_i \lambda_i^*\big(\ket{v}, \ket{w_i}\big)^*  = \sum_i \lambda_i^*\big(\ket{w_i}, \ket{v}\big)$$
\end{solution}




\subsection{Exercise} Verify that $\ket{w}\equiv(1,1)$ and $\ket{v}\equiv (1,-1)$ are orthogonal. What are the normalized forms of these vectors?

\begin{solution}
    To verify compute the inner product $$\braket{w}{v}  = \begin{bmatrix} 1 & 1 \end{bmatrix} 
    \begin{bmatrix}  1 \\ -1 \end{bmatrix} = 1 - 1 = 0$$ so the vectors are orthogonal and to normalize take $\frac{\ket{w}}{\norm{\ket{w}}} $ where  $\norm{\ket{w}} = \sqrt{\braket{w}} = \sqrt{2}$ then $\frac{\ket{w}}{\norm{\ket{w}}} = \frac{1}{\sqrt{2}}(1,1)$ and $\norm{\ket{v}} = \sqrt{\braket{v}} = \sqrt{2}$ so $\frac{\ket{v}}{\norm{\ket{v}}} = \frac{1}{\sqrt{2}}(1,-1)$ thus $\frac{1}{\sqrt{2}}(1,1), \quad \frac{1}{\sqrt{2}}(1,-1)$ are the normalized forms of $\ket{w}$ and $\ket{v}$ respectively.
\end{solution}




\subsection{Exercise} Prove that the Gram-Schmidt procedure produces an orthonormal basis for $V$. 

\begin{solution}
Starting with $\ket{w_1},...,\ket{w_d}$ as a basis for vector space $V$  consider for $1\leq k \leq d - 1$ a subspace $V_k =$ Span$\{\ket{w_1},...,\ket{w_k}\}$.

Now to construct an orthogonal basis set $\ket{u_1} = \ket{w_1}$ so Span$\{\ket{u_1}\}=$ Span$\{\ket{w_1}\}$ continuing inductively for some $k < d-1$ an orthogonal basis has been constructed by $\ket{u_1},...,\ket{u_k}$ by defining $\ket{u_{k+1}} = \ket{w_{k+1}}-\sum^k_{i=1}\braket{u_i}{w_{k+1}}\ket{u_i}$ for $V_k$ by removing each non-orthogonal component of all previous vectors already in the orthogonal basis, i.e. by the Orthogonal Decomposition Theorem $\ket{u_{k+1}}$ is orthogonal to $V_k$ and $\ket{w_{k+1}} \in V_{k+1} $ then since $V_{k+1}$ is closed under subtraction $\implies \ket{u_{k+1}} \in V_{k+1}$. 

Note that $\ket{u_{k+1}} \neq 0$ since $\ket{w_{k+1}} \notin V_k$ so $\ket{u_1},...,\ket{u_k+1}$ is an orthogonal set of vectors forming a basis for $V_{k+1}$. Halt the procedure when $V_k \equiv V$.

To make the set orthonormal set $\ket{v_1} = \ket{u_1}/\norm{\ket{u_1}}$ and normalize each vector $\ket{v_{k+1}} = \ket{u_{k+1}}/\norm{\ket{u_{k+1}}}$ which finishes the proof for Gram-Schmidt producing an orthonormal basis for $V$. By putting it all together results in 2.17 as  $$
\ket{v_{k+1}} \equiv \frac{\ket{w_{k+1}} - \sum^k_{i=1}\braket{v_i}{w_{k+1}}\ket{v_i}}{\norm{\ket{w_{k+1}} - \sum^k_{i=1}\braket{v_i}{w_{k+1}}\ket{v_i}}}$$ 
\end{solution}




\subsection{Exercise} 
\textbf{(Pauli operators and the outer product)} The Pauli matrices can be considered as operators with respect to an orthonormal basis $\ket{0}, \ket{1}$ for a two-dimensional Hilbert space. Express each of the Pauli operators in the outer product notation.

\begin{solution}
    $$\sigma_0 \equiv I \equiv 
    \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \equiv 
    \ket{0}\bra{0} + \ket{1}\bra{1}, \quad
    \sigma_1 \equiv X \equiv 
    \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \equiv 
    \ket{0}\bra{1} + \ket{1}\bra{0}
    $$ 
    $$
    \sigma_2 \equiv Y \equiv 
    \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} \equiv 
    i\ket{0}\bra{1} -i\ket{1}\bra{0}, \quad
    \sigma_3 \equiv Z \equiv 
    \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \equiv 
    \ket{0}\bra{0} - \ket{1}\bra{1}
    $$
    which completes the 4 Pauli matrices listed in 2.2.
\end{solution}




\subsection{Exercise} Suppose $\ket{v_i}$ is an orthonormal basis for an inner product space $V$. What is the matrix representation for the operator $\ket{v_j}\bra{v_k}$, with respect to the $\ket{v_i}$ basis?

\begin{solution}
    Applying the operator to the orthonormal basis find $\ket{v_j}\braket{v_k}{v_i}$ which equals 0 where $k \neq i$ and equals $\ket{v_j}$ where $k = i$ thus $\ket{v_j}\bra{v_k}$ is all zeroes except in row  $j$ and column $k$.
\end{solution}




\subsection{Exercise} \textbf{(Eigendecomposition of the Pauli matrices)} Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.

\begin{solution}
    For $X$'s eigenvalues and eigenvectors find:
    $$\det(X-I\lambda) = \lambda^2 -1 = 0 \implies \lambda = \pm 1, 
    \quad (X - I)\ket{v} = \textbf{0} \implies \ket{v_1} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, 
    (X+I)\ket{v} = \textbf{0} \implies \ket{v_2} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$$
    then the orthonormal set of eigenvectors is $\ket{v_1} \mapsto \ket{+} = \frac{1}{\sqrt{2}}\ket{v_1}$ and $\ket{v_2} \mapsto \ket{-}=\frac{1}{\sqrt{2}}\ket{v_2}$ so its diagonal representation is given by $X = \sum_i \lambda_i\ket{i}\bra{i} = (1)\ket{+}\bra{+} + (-1)\ket{-}\bra{-} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
    $ which is diagonal in the $\{\ket{+},\ket{-}\}$ basis.

    For $Y$'s eigenvalues and eigenvectors find:
    $$\det(Y-I\lambda) = \lambda^2 - 1 = 0 \implies \lambda = \pm 1, \quad 
    (Y - I)\ket{v} = \textbf{0} \implies \ket{v_1} = \begin{bmatrix} 1 \\ i \end{bmatrix}, 
    (Y+I)\ket{v} = \textbf{0} \implies \ket{v_2} = \begin{bmatrix} 1 \\ -i \end{bmatrix}
    $$
    again the orthonormal set of eigenvectors is $\ket{v_1} \mapsto \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ i \end{bmatrix}$ and $\ket{v_2} \mapsto \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -i \end{bmatrix}$ so its diagonal representation is given by $Y = \sum_i \lambda_i\ket{i}\bra{i} = (1)\ket{v_1}\bra{v_1} + (-1)\ket{v_2}\bra{v_2} = \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}
    $.

    For $Z$'s eigenvalues and eigenvectors find:
    $$\det(Z-I\lambda) = \lambda^2 - 1 = 0 \implies \lambda = \pm 1, \quad 
    (Z - I)\ket{v} = \textbf{0} \implies \ket{v_1} = \ket{0}, 
    (Z+I)\ket{v} = \textbf{0} \implies \ket{v_2} = \ket{1}
    $$
    where $\ket{0}, \ket{1}$ are already orthonormal then its diagonal representation $Z = \sum_i \lambda_i\ket{i}\bra{i} = (1)\ket{0}\bra{0} + (-1)\ket{1}\bra{1} = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
    $.
\end{solution}



\subsection{Exercise} Prove that the matrix $\begin{bmatrix}
    1 & 0 \\ 1 & 1
\end{bmatrix}$ is not diagonalizable. 

\begin{solution}
    An operator is diagonalizable if and only if it has a diagonal representation $A = \sum_i \lambda_i\ket{i}\bra{i}$ where each $\ket{i}$ form an eigenbasis. Now proceeding to the eigenvalues and eigenvectors of the given matrix are 
    $$\det\begin{bmatrix}
        1-\lambda & 0 \\ 1 & 1-\lambda
    \end{bmatrix} = (1-\lambda)^2= 0 \implies \lambda = 1 \text{ with eigenvector } \ket{1}$$ because the matrix only has one eigenvector a complete eigenbasis cannot be formed, for a $2\cross 2$ matrix two linear independent eigenvectors are needed. Thus the matrix is not diagonalizable. 
\end{solution}



\subsection{Exercise} If $\ket{w}$ and $\ket{v}$ are any two vectors, show that $(\ket{w}\bra{v})^\dagger = \ket{v}\bra{w}$.

\begin{solution}
    Apply the rule $(AB)^\dagger = B^\dagger A^\dagger$ to the given outer product $(\ket{w}\bra{v})^\dagger= (\bra{v})^\dagger(\ket{w})^\dagger = \ket{v}\bra{w}$ which was to be shown. 
\end{solution}



\subsection{Exercise} \textbf{(Anti-linearity of the adjoint)} Show that the adjoint operation is anti-linear,
$$
\bigg(\sum_ia_i A_i\bigg)^\dagger = \sum_i a^*_i A^\dagger_i$$

\begin{solution}
    Let $A =\sum_ia_i A_i $ then it follows that $A^\dagger = \big(\sum_ia_i A_i\big)^\dagger$ now apply 2.32 to find
    $$(A^\dagger\ket{v}, \ket{w}) = \big((\sum_ia_i A_i)^\dagger\ket{v}, \ket{w}\big)  = (\ket{v}, \sum_ia_i A_i\ket{w}) 
    \quad\quad\quad $$ $$ \quad\quad\quad = \sum_ia_i(\ket{v}, A_i\ket{w}) = \sum_ia_i(A_i^\dagger \ket{v}, \ket{w})  = (\sum_ia_i^*A_i^\dagger \ket{v}, \ket{w}) $$ noting that the left side ("bra" side) takes the complex conjugate of $a_i$. This is because the inner product is linear in the second argument but anti-linear in the first.
\end{solution}



\subsection{Exercise} Show that $(A^\dagger)^\dagger=A$.

\begin{solution}
    Apply 2.32 twice to find $(\ket{v}, (A^\dagger)^\dagger\ket{w}) = (A^\dagger\ket{v}, \ket{w}) = (\ket{v}, A\ket{w})$ thus $(A^\dagger)^\dagger=A$ because there exists a \textbf{unique} linear operator $A^\dagger$ on $V$. This uniqueness forces $(A^\dagger)^\dagger=A$.
\end{solution}



\subsection{Exercise} Show that any projector $P$ satisfies the equation $P^2 = P$

\begin{solution}
    By definition $P = \sum_{i=1}^k\ket{i}\bra{i}$ then $$P^2 = \sum_{i=1}^k\ket{i}\bra{i}\sum_{j=1}^k\ket{j}\bra{j}= \sum_{i=1}^k\sum_{j=1}^k\ket{i}\braket{i}{j}\bra{j} = 
    \sum_{i=1}^k\sum_{j=1}^k\ket{i}\delta_{ij}\bra{j} = \sum_{i=1}^k\ket{i}\bra{i} = P$$

    Noting that $\delta_{ij} = 0$ unless $i=j$.
    
\end{solution}



\subsection{Exercise} Show that a normal matrix is Hermitian if and only if it has real eigenvalues.

\begin{solution}

    If a matrix $A$ is normal and Hermitian(self-adjoint) then $AA^\dagger = A^\dagger A$ and $A = A^\dagger$ since the operator is normal by the spectral decomposition theorem it can be written as $A = \sum_i \lambda_i\ket{i}\bra{i}$ and $A^\dagger= \sum_i \lambda_i^*\ket{i}\bra{i}$ but because $A$ is self-adjoint $$\sum_i \lambda_i\ket{i}\bra{i}= \sum_i \lambda_i^*\ket{i}\bra{i} \implies \lambda = \lambda^*$$
    Then for $\lambda := a + bi \in \C$ the implication is $a + bi = a - bi \implies b = 0$ so the eigenvalues have no complex component thus they are real eigenvalues. $\square$

    Converse:

    If the matrix $A$ is normal and has real eigenvalues then by the spectral decomposition theorem it can be written as $A = \sum_i \lambda_i\ket{i}\bra{i}$ and with real eigenvalues $\lambda = \lambda^*$ so  $$A = \sum_i \lambda_i\ket{i}\bra{i} = \sum_i \lambda_i^*\ket{i}\bra{i} = A^\dagger$$
    which implies $A$ is Hermitian. $\square$
\end{solution}



\subsection{Exercise} Show that all eigenvalues of a unitary matrix have modulus 1, that is, can be written in the form $e^{i\theta}$ for some real $\theta$.

\begin{solution}
    For a given unitary matrix $U$ write $U = \sum_i\lambda_i\ket{w_i}\bra{v_i}$ for two orthonormal bases $\ket{w_i},\ket{v_i}$. Using the property $UU^\dagger = I$ then combining the above gives $$
    UU^\dagger = \bigg(\sum_i\lambda_i\ket{w_i}\bra{v_i}\bigg)\bigg(\sum_i\lambda_i\ket{w_i}\bra{v_i}\bigg)^\dagger = \sum_i\lambda_i\ket{w_i}\bra{v_i}\sum_i\lambda_i^*(\ket{w_i}\bra{v_i})^\dagger = \quad$$
    $$\quad \sum_i\lambda_i\ket{w_i}\bra{v_i}\sum_i\lambda_i^*\ket{v_i}\bra{w_i}=\sum_i\sum_i\lambda_i\lambda_i^*\ket{w_i}\braket{v_i}\bra{w_i} = \sum_i|\lambda_i|^2\ket{w_i}\bra{w_i} = I$$

    Noting that the second sum can be dropped because any time the indices of the two sums are different they produce 0. This also implies for each eigenvalue $|\lambda_i|^2 = 1 \implies |\lambda_i|=1$ so then it can be written as $\lambda_i\lambda^*_i  = e^{i\theta}e^{-i\theta}=e^0 = 1$ for some real $\theta$ because any complex number mod 1 can be written $e^{i\theta}$.
\end{solution}




\subsection{Exercise} \textbf{(Pauli matrices: Hermitian and unitary)} Show that the Pauli matrices are Hermitian and unitary. 

\begin{solution}
For matrices $I,X,Y$, and $Z$ find the following 
    $$II^\dagger = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix}\begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = I, \quad 
    XX^\dagger = \begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = I$$
    $$
    YY^\dagger = \begin{bmatrix}0 & -i \\ i & 0 \end{bmatrix}\begin{bmatrix}0 & -i \\ i & 0 \end{bmatrix}=\begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix}=I, \quad 
    ZZ^\dagger = \begin{bmatrix}1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix}1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = I$$

    Note that in all cases each matrix equals its adjoint (transpose then conjugate) and they all satisfy the unitary requirement.
\end{solution}



\subsection{Exercise} \textbf{(Basis changes)} 
Suppose $A^\prime$ and $A^{\prime\prime}$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\ket{v_i}$ and $\ket{w_i}$. Then the elements of $A^\prime$ and $A^{\prime\prime}$ are $A^\prime_{ij} = \bra{v_i}A\ket{v_j}$ and $A^{\prime\prime}_{ij} = \bra{w_i}A\ket{w_j}$. Characterize the relationship between $A^\prime$ and $A^{\prime\prime}$.

\begin{solution}
    Since $U$ is defined as the outer product between two orthonormal bases sets $U \equiv \sum_i\ket{w_i}\bra{v_i} $ $\implies U_{ij} = \braket{v_i}{w_j}$ (Note this is by choice as $U\equiv \sum_i\ket{v_i}\bra{w_i}$ is valid as long as the choice is consistent) then using this  $$
     A^\prime_{ij} = \bra{v_i}A\ket{v_j} = \bra{v_i}IAI\ket{v_j} = \bra{v_i}\sum_k\ket{w_k}\bra{w_k}A\sum_l\ket{w_l}\bra{w_l}\ket{v_j} = \sum_k\sum_l\bra{v_i}\ket{w_k}\bra{w_k}A\ket{w_l}\bra{w_l}\ket{v_j}$$
     $$
     = \sum_k\sum_l\bra{v_i}\ket{w_k}A^{\prime\prime}_{kl}\bra{w_l}\ket{v_j}= \sum_k\sum_lU_{ik}A^{\prime\prime}_{kl}\bra{v_j}\ket{w_l}^\dagger = \sum_k\sum_lU_{ik}A^{\prime\prime}_{kl}U_{jl}^\dagger$$
     More succinctly this can be written as $A^\prime = UA^{\prime\prime} U^\dagger$ which characterizes the matrix representations relationship. 
\end{solution}



\subsection{Exercise} Repeat the proof of the spectral decomposition in Box 2.2 for the case when $M$ is Hermitian, simplifying the proof wherever possible. 


\begin{solution}

    \textbf{Spectral Decomposition} - \textit{Any normal operator $M$ on a vector space $V$ is diagonal with respect to some orthonormal basis for $V$. Conversely, any diagonalizable operator is normal.}

    In this case $M$ is Hermitian so $M = M^\dagger$. 

    For $d= 1$, $M$ is obviously diagonal. Let $\lambda$ be an eigenvalue of $M$, $P$ a projector onto the $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal complement. Expanding $$M = IMI = (P+Q)M(P+Q) = (PM +QM)(P+Q)=PMP + QMP + PMQ + QMQ$$
    Where $QMP = PMQ = 0$, $\quad PMP = \lambda P$, and $QMQ$ is Hermitian then $QMQQM^\dagger Q=QM^\dagger QQM Q$ since $M = M^\dagger$ which gives
    $$M = \lambda P  +QMQ$$

    Then since $QMQ$ is diagonal with respect to an orthonormal basis for subspace $Q$ and $\lambda P$ is already diagonal then the sum is diagonal with respect to some orthonormal basis for $V$. Thus $M$ can be written as $\sum_i \lambda_i \ket{i}\bra{i}$ which was to be shown. $\square$

    For the converse, since $M$ is Hermitian it is normal. 
\end{solution}



\subsection{Exercise} Prove that two eigenvectors of a Hermitian operator with different eigenvalues are necessarily orthogonal.

\begin{solution}
    Let $\ket{v_1}$ and $\ket{v_2}$ be two eigenvectors of a Hermitian operator $M$ corresponding to different eigenvalues then by definition
    $$
    M\ket{v_1} = \lambda_1\ket{v_1} \quad \text{ and } \quad M\ket{v_2} = \lambda_2\ket{v_2}$$
    since $M$ is Hermitian by the spectral decomposition theorem $M = \sum_i \lambda_i \ket{i}\bra{i}$ then for two different eigenvalues $\lambda_1$ and $\lambda_2$, $\ket{i}\bra{i}\ket{v_1} = \ket{v_1} \iff \ket{i}=\ket{v_1}$ since eigenvectors only live in one projectors range thus for two different eigenvalues $$\bra{v_1}\ket{i}\bra{i}\ket{j}\bra{j}\ket{v_2}= 0 \iff i \neq j$$ so $\ket{v_1}$ and $\ket{v_2}$ must be orthogonal. $\square$
    
\end{solution}




\subsection{Exercise} Show that the eigenvalues of a projector $P$ are all either 0 or 1.

\begin{solution}
    First note that for a diagonal matrix the eigenvalues are just the entries along the diagonal of the matrix. Since $P \equiv \sum^k_{i=1} \ket{i}\bra{i}$ it is already diagonal and is essentially $I$ in a $k$-dimensional subspace. Denoting the full vector space as $d$-dimension, then $P$'s characteristic polynomial is  $\det(P - \lambda I ) = (1-\lambda)^k(-\lambda^{d-k}) = 0 $ thus $\lambda =$ 0 or 1. 
\end{solution}



\subsection{Exercise} \textbf{(Hermiticity of positive operators)} Show that a positive operator is necessarily Hermitian. (\textit{Hint:} Show that an arbitrary operator $A$ can be written $A=B+iC$ where $B$ and $C$ are Hermitian.)

\begin{solution}
    A positive operator $A$ is defined that for any vector $\ket{v}$, $\quad \bra{v}A\ket{v} \in \R_{\geq0}$. Following the hint split $A$, into $A = A/2 + A/2$ and add 0 to get $$A = \frac{A}{2} + \frac{A}{2} + \frac{A^\dagger}{2} - \frac{A^\dagger}{2} = \frac{A}{2} + \frac{A^\dagger}{2} + \frac{i}{i}\bigg(\frac{A}{2} - \frac{A^\dagger}{2}\bigg) \implies \quad B = \frac{A+A^\dagger}{2}, \quad C = \frac{A-A^\dagger}{2i}$$
    So $A = B + iC$ then using the linearity in the second argument of the inner product gives $$\bra{v}A\ket{v} = \bra{v}B + iC\ket{v} = \bra{v}B\ket{v} + \bra{v}iC\ket{v} \implies \quad C = \textbf{0}$$ with the implication coming from the fact that the result of the inner product is real. Now checking if $B$ is Hermitian to find
    $$
    B^\dagger = \bigg(\frac{A+A^\dagger}{2}\bigg)^\dagger = \frac{A^\dagger+A}{2}= \frac{A+A^\dagger}{2} = B$$ thus $B$ is Hermitian and $A = B $ therefore $A$ is Hermitian. $\square$
    
\end{solution}




\subsection{Exercise} Show that for any operator $A, \quad A^\dagger A $ is positive.

\begin{solution}
    Consider the following 
    $$\bra{v} A^\dagger A \ket{v}= \braket{Av}{Av}  = \norm{A\ket{v}}^2 \geq 0$$
    where the squared norm is always real and non-negative.
\end{solution}



\subsection{Exercise} Let $\ket{\psi}= (\ket{0}+\ket{1})/\sqrt{2}$. Write out $\ket{\psi}^{\otimes2}$ and $\ket{\psi}^{\otimes3}$ explicitly, both in terms of tensor products like $\ket{0}\ket{1}$, and using the Kronecker product. 

\begin{solution}
    $$\ket{\psi}^{\otimes2} = \frac{(\ket{0}+\ket{1})}{\sqrt{2}}\otimes \frac{(\ket{0}+\ket{1})}{\sqrt{2}} = \frac{1}{2}\big[ (\ket{0}+\ket{1})\ket{0} + (\ket{0}+\ket{1})\ket{1}  \big]= \frac{1}{2}\big[ \ket{0}\ket{0}+\ket{1}\ket{0} + \ket{0}\ket{1}+\ket{1}\ket{1}  \big]$$

    Continuing to $\ket{\psi}^{\otimes 3}$ as $\ket{\psi}^{\otimes2}\ket{\psi} $ find
    $$
    \ket{\psi}^{\otimes 3} = \frac{1}{2}\big[ \ket{0}\ket{0}+\ket{1}\ket{0} + \ket{0}\ket{1}+\ket{1}\ket{1}  \big]\otimes\frac{(\ket{0}+\ket{1})}{\sqrt{2}} 
    = \frac{1}{\sqrt{8}}\big[\ket{0}\ket{0}\ket{0} + \ket{0}\ket{0}\ket{1} + \ket{0}\ket{1}\ket{0} + ... + \ket{1}\ket{1}\ket{1}]
    $$
    Where counting in binary fills in the ellipsis (the "...") above. Onto the Kronecker product 
    $$\ket{\psi}^{\otimes2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} \otimes 
     \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2}\begin{bmatrix}
         1 \\ 1 \\ 1 \\ 1
     \end{bmatrix}, \quad  \ket{\psi}^{\otimes 3} = \frac{1}{2}\begin{bmatrix}
         1 \\ 1 \\ 1 \\ 1
     \end{bmatrix} \otimes \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{\sqrt{8}} \begin{bmatrix}
         1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1
     \end{bmatrix}
    $$
\end{solution}




\subsection{Exercise} Calculate the matrix representation of the Pauli operators (a) $X$ and $Z$; (b) $I$ and $X$; (c) $X$ and $I$. Is the tensor product commutative?

\begin{solution}
    Recall $$I = \begin{bmatrix} 1 & 0 \\ 0&1  \end{bmatrix}, \quad X = \begin{bmatrix} 0&1 \\ 1 &0\end{bmatrix}, \quad Z = \begin{bmatrix} 1&0 \\ 0&-1 \end{bmatrix}   $$ then the following are their tensor products 
    \begin{enumerate}
        \item $$X \otimes Z = \begin{bmatrix} \textbf{0} & Z \\ Z & \textbf{0} \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & -1  \\ 1 & 0 & 0 & 0 \\0 & -1 & 0 & 0 \end{bmatrix} $$
        \item $$I \otimes X = \begin{bmatrix} X & \textbf{0}  \\  \textbf{0} & X\end{bmatrix} = \begin{bmatrix} 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0  \\ 0 & 0 & 0 & 1 \\0 & 0 & 1 & 0 \end{bmatrix} $$
        \item $$X \otimes I = \begin{bmatrix} \textbf{0} & I \\ I & \textbf{0} \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1  \\ 1 & 0 & 0 & 0 \\0 & 1 & 0 & 0 \end{bmatrix} $$
    \end{enumerate}

    Note that case (b) and case (c), $X\otimes I \neq I \otimes X$ do not match which provides an example that the tensor product is not commutative. 
\end{solution}



\subsection{Exercise} Show that the transpose, complex conjugation, and adjoint operations distribute over the tensor product, $$
(A \otimes B)^*= A^* \otimes B^*; \quad
(A\otimes B)^T = A^T\otimes B^T; \quad
(A \otimes B)^\dagger = A^\dagger \otimes B^\dagger$$

\begin{solution}
    Expanding out into the matrix form find 
    $$
    (A\otimes B)^*  = \begin{bmatrix}
        a_{11}B & \cdots & a_{1n}B \\
        \vdots &  \ddots & \vdots  \\
        a_{m1}B & \cdots  & a_{mn}B
    \end{bmatrix}^* 
    = \begin{bmatrix}
        (a_{11}B)^* & \cdots & (a_{1n}B)^* \\
        \vdots &  \ddots & \vdots  \\
        (a_{m1}B)^* & \cdots  & (a_{mn}B)^*
    \end{bmatrix} 
    = \begin{bmatrix}
        a_{11}^*B^* & \cdots & a_{1n}^*B^* \\
        \vdots &  \ddots & \vdots  \\
        a_{m1}^*B^* & \cdots  & a_{mn}^*B^*
    \end{bmatrix} =  A^* \otimes B^*$$

    $$(A\otimes B)^T 
    = \begin{bmatrix}
        a_{11}b_{11} & \cdots & a_{1n}b_{1q} \\
        \vdots &  \ddots & \vdots  \\
        a_{m1}b_{p1} & \cdots  & a_{mn}b_{pq}
    \end{bmatrix}^T 
    = \begin{bmatrix}
        a_{11}b_{11} & \cdots &  a_{m1}b_{p1}\\
        \vdots &  \ddots & \vdots  \\
        a_{1n}b_{1q} & \cdots  & a_{mn}b_{pq}
    \end{bmatrix}
    = \begin{bmatrix}
        a_{11}B^T & \cdots &  a_{m1}B^T\\
        \vdots &  \ddots & \vdots  \\
        a_{1n}B^T & \cdots  & a_{mn}B^T
    \end{bmatrix} =  A^T \otimes B^T$$

    Combine the above two distributive properties to find the adjoint operation distributes: 

    $\quad (A \otimes B)^\dagger = [(A \otimes B)^T]^*
    = [A^T \otimes B^T]^* = A^{T*} \otimes B^{T*}
    = A^\dagger \otimes B^\dagger$
\end{solution}



\subsection{Exercise} Show the tensor product of two unitary operators is unitary.

\begin{solution}
    Let matrices $U_{m\cross m}$ and $W_{n\cross n}$ be \textit{unitary} then $UU^\dagger = I$ and $WW^\dagger=I$. Their tensor product $(U \otimes W)$ is unitary if $(U \otimes W)(U \otimes W)^\dagger = I_{mn\cross mn}$ with the identity matrix being the larger size so using 2.28 and equation (2.45) find
    $$(U \otimes W)(U \otimes W)^\dagger=(U \otimes W)(U^\dagger \otimes W^\dagger)=UU^\dagger \otimes WW^\dagger= I_{m\cross m}\otimes I_{n\cross n}= I_{mn\cross mn}$$

    which was to be shown. $\square$
\end{solution}



\subsection{Exercise} Show that the tensor product of two Hermitian operators is Hermitian. 

\begin{solution}
     Let matrices $A_{m\cross m}$ and $B_{n\cross n}$ be \textit{Hermitian} then $A = A^\dagger$ and $B = B^\dagger$. Their tensor product $(A \otimes B)$ is Hermitian if $(A \otimes B) = (A \otimes B)^\dagger$ so using 2.28 find 
     $$
     (A \otimes B) = (A^\dagger \otimes B^\dagger) = (A \otimes B)^\dagger$$
     which was to be shown. $\square$
\end{solution}








\end{document}
