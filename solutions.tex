\documentclass[12pt]{exam}

\usepackage[
  letterpaper,
  margin=0.3in,
  includehead,    % include the header area in the 0.25in margin
  includefoot     % include the footer area in the 0.25in margin
]{geometry}
\usepackage[shortlabels]{enumitem}
\setlist[enumerate,1]{label=(\alph*)} %makes list alphabetic

\usepackage{physics}
\usepackage{amsfonts}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\usepackage{pgfplots} % For sketching graphs
\pgfplotsset{compat=1.18}


\usepackage{caption}

\usepackage[hidelinks]{hyperref}

\makeatletter
  % Save the original \section into \EXAM@orig@section
  \let\EXAM@orig@section\section
  \renewcommand{\section}[1]{
    \refstepcounter{section}
    \EXAM@orig@section*{#1~\thesection}
    
    \addcontentsline{toc}{section}{#1~\thesection}
  }
\makeatother

\usepackage{etoolbox}

\newcounter{exercise}[section]
\renewcommand{\theexercise}{\thesection.\arabic{exercise}}
\newcounter{problem}[section]
\renewcommand{\theproblem}{\thesection.\arabic{problem}}

\makeatletter
  \let\EXAM@orig@subsection\subsection

  %Redefine \subsection itself:
  \renewcommand{\subsection}[1]{
    \ifstrequal{#1}{Exercise}{
      \refstepcounter{exercise}
      \EXAM@orig@subsection*{#1~\theexercise}
      \addcontentsline{toc}{subsection}{#1~\theexercise}
    }{
      \ifstrequal{#1}{Problem}{
        \refstepcounter{problem}
        \EXAM@orig@subsection*{#1~\theproblem}
        \addcontentsline{toc}{subsection}{#1~\theproblem}
      }{
        \refstepcounter{subsection}%
        \EXAM@orig@subsection*{#1~\thesubsection}%
        \addcontentsline{toc}{subsection}{#1~\thesubsection}
      }
    }
  }
\makeatother

\usepackage{hanging}
\usepackage{parskip}



\title{%
  \centering
  Quantum Computation and Quantum Information\\
  Solution Manual\\
  \small
  For Michael A. Nielsen and Isaac L. Chuang’s 10th Anniversary Edition
}

\author{By Justin Beltran}
\date{May 2025}

\begin{document}

\maketitle


\firstpagefooter{}{}{By Justin Beltran}
\runningfooter{}{}{By Justin Beltran}
\printanswers

\newpage


\section{Chapter}
\subsection{Exercise}
\textbf{(Probabilistic classical algorithm)} Suppose that the problem is not to distinguish between the constant and balanced functions \textit{with certainty}, but rather, with some probability of error $\epsilon < 1/2$. What is the performance of the best classical algorithm for this problem?

\begin{solution}
    The best classical algorithm for Deutsch's problem with 100\% success would require $2^{n-1}+1$ queries because Alice may receive $2^{n-1}$ 0's before receiving a 1. The best probabalistic classical algorithm for the same problem with $\epsilon < 1/2$ would be 3 queries. The first query establishes a baseline and the second has probability 1/2 of matching the first but 3 queries would have probability 1/4. Generalizing this for $q$ queries with probability of error $\epsilon$ is $$\epsilon < \frac{1}{2^{q-1}} = 2^{-q+1} \quad \implies \quad \log\frac{1}{\epsilon} + 1 < q \quad \implies \quad O\big(\log\frac{1}{\epsilon}\big)$$
\end{solution}



\subsection{Exercise}
Explain how a device which, upon input of one of two non-orthogonal quantum states $\ket{\psi}$ or $\ket{\varphi}$ correctly identified the state, could be used to build a device which cloned the states $\ket{\psi}$ and $\ket{\varphi}$, in violation of the no-cloning theorem. Conversely, explain how a device for cloning could be used to distinguish non-orthogonal quantum states.

\begin{solution}
    \begin{enumerate}
        \item Given a device that correctly identifies one of two non-orthoganal quantum states, read one of the unknown states in and now there is a perfect classical "image" of the state. Now use that image to prepare the exact same state in the quantum device as many times as desired violating the no-cloning theorem. 

        \item A device that clones quantum states could distinguish non-orthogonal quantum states by setting up $many$ clones then measuring all the clones to reconstruct the quantum state perfectly.
    \end{enumerate}
\end{solution}



\subsection{Problem} 
\textbf{(Feynman - Gates conversation)}
Construct a friendly imaginary discussion of about 2000 words between Bill Gates and Richard Feynman, set in the present, on the future of computation. (\textit{Comment: }You might like to try waiting until you’ve read the rest of the book before attempting this question. See the ‘History and further reading’ below for pointers to one possible answer for this question.)

\begin{solution}
For brevity, this was shortened well below 2000 words:\\
Gates: Isn’t the way you do computing different?\\
Feynman: Yes, rooted in physics and math but computers perform huge calculations and sometimes suggest unconsidered ideas.\\
Gates: How do you view the difference between your work in the war and that now?\\
Feynman: We did what was necessary to win the war. Afterwards, many of us questioned the bomb’s impact. I returned to physics at Caltech, but I learned that even simple machines can yield vital results.\\
Gates: You predicted unseen computers by 2050.\\
Feynman: I studied the physical limits of computation and found no quantum mechanical barrier to miniaturization—only considerations of thermodynamics and reversibility. If you want atomic‐scale machines, you must use quantum mechanics rather than classical physics.\\
Gates: Why derive Shannon’s theorem three different ways?\\
Feynman: Deriving known results independently teaches you new insights. \\
Gates: At Microsoft, we focus on usability and extending human cognition.\\
Feynman: I’m interested. In physics, my Feynman diagrams simplified months of work into an evening. Tools that simplify complexity and reveal new layers are invaluable.\\
Gates: How do you anticipate computing’s future?\\
Feynman: We need computers modeled on natural laws so programs adapt gracefully to changing boundary conditions, reducing unforeseen errors.\\
Gates: Does that mean programmers must be physicists?\\
Feynman: Fundamental physical laws are simple; knowing some physics helps, but it isn’t mandatory.\\
Gates: How would nature-based model software affect testing?\\
Feynman: Testing becomes more straightforward—failures can reveal new phenomena rather than just bugs.\\
Gates: So software can be “quantized”?\\
Feynman: Yes—small, self‐contained modules that solve specific problems.\\
Gates: Good software engineers draw inspiration across disciplines.\\
Feynman: Agreed—like in physics, varied perspectives uncover truths.\\
Gates: Your most interesting work since your Nobel Prize?\\
Feynman: Theory of liquid helium, laws of weak interaction, parton theory. Now I’m just playing with new ideas.\\
Gates: Any disappointments?\\
Feynman: Turbulence and superconductivity remain unsolved.  I failed but tackling them was rewarding.\\
Gates: Complexity’s big question?\\
Feynman: How simple rules give rise to complex behavior from bee swarms to neural networks. It demands interdisciplinary study.\\
Gates: Will you consult for Microsoft?\\
Feynman: That’s the “wackiest idea” I’ve heard! Perhaps. A software revolution is coming; imagination is key.
\end{solution}



\subsection{Problem} What is the most significant discovery yet made in quantum computation and quantum information? Write an essay of about 2000 words for an educated lay audience about the discovery. (\textit{Comment}: As for the previous problem, you might like to try waiting until you’ve read the rest of the book before attempting this question.)

\begin{solution}
Again for brevity, this was shortened well below 2000 words:\\
    The most important discovery that has been made in quantum computation and quantum information I believe is the qubit $\ket{\psi}$. Without high-level quantum computers being widely available, algorithm designers or more generally theorists could not continue to research or develop ideas without the mathematical formalism of the qubit. That is the the notion of a fundamental unit of quantum information is a two-level quantum system, i.e. the qubit, rather than a classical bit or bitstring. Further that this qubit can exist in a coherent superposition $\ket{\psi} = \alpha \ket{0} + \beta\ket{1}$ and again more generally that $\alpha$ and $\beta$ can be any complex coefficient in $\C$ that obey $|\alpha|^2 + |\beta|^2  = 1$. This laid the groundwork for encoding information in quantum amplitudes and opened the door to phenomena such as entanglement or interference which have no classical analogue. These additional processes are where the most significant algorithms like Shor's or Grover's extract most of their power. 
    
    This formalism of the qubit also extends nicely to
    many body systems via the tensor product $\otimes$ where two qubit's, $\ket{\psi}$ and $\ket{\phi}$, combined state can be written $\ket{\psi} \otimes \ket{\phi}$. From here the idea of the 2-Dimensional Hilbert Space $\mathcal{H_2} = \C^2$ arises and by choosing an orthonormal basis: $\{\ket{0}, \ket{1}\}$, a diagonal basis: $\{\ket{+}, \ket{-}\}$ is generated where  $\ket{+} = (\ket{0} + \ket{1})/\sqrt{2}, \quad \ket{-} = (\ket{0} - \ket{1})/\sqrt{2}$. Combining this concept with the previously mentioned tensor product the Hilbert Space grows with the number of these qubits to $\mathcal{H} = (\C^2)^{\otimes n}$ for $n$ qubits. Which provides enough mathematical foundation for the ideas of quantum computation and quantum information to be explored before the arrival of widespread quantum hardware.
\end{solution}


\vspace{12pt}
\section{Chapter}

\subsection{Exercise}
\textbf{(Linear dependence: example)} Show that (1, -1), (1, 2) and (2, 1) are linearly dependent.

\begin{solution}
    They are linearly dependent if a set $a_1,..., a_n \in \C$ with $a_i \neq 0$ for at least one value of $i$ s.t. $a_1\ket{v_1} + a_2\ket{v_2} + a_3\ket{v_3} = 0$ now choose $a_1 = 1, a_2 = 1, a_3 =-1$ and observe that $$\begin{bmatrix}  1 \\ -1 \end{bmatrix} + \begin{bmatrix}  1 \\ 2 \end{bmatrix} - \begin{bmatrix}  2 \\ 1 \end{bmatrix} = 0 $$ Note this is generalizable to any $x \in \C$ where $a_1 = x, a_2 = x, a_3 =-x$.
\end{solution}



\subsection{Exercise} \textbf{(Matrix representations: example)} Suppose V is a vector space with basis vectors $\ket{0}$ and $\ket{1}$, and $A$ is a linear operator from $V$ to $V$ such that $A\ket{0} = \ket{1}$ and  $A\ket{1} = \ket{0}$. Give a matrix representation for $A$, with respect to the input basis $\ket{0}, \ket{1}$, and the output basis $\ket{0}, \ket{1}$. Find input and output bases which give rise to a different matrix representation of $A$.

\begin{solution}
    For vector space $V$ with basis $\ket{0}, \ket{1}$ and $A: V \mapsto V$ as described then 
    $$ A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \implies \quad \begin{bmatrix} a_1 & a_2  \\ a_3 & a_4 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} a_1 \\ a_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$$ which implies $a_1 = 0, a_3 = 1$ then using $A\ket{1} = \ket{0}$ find 
    $$A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \implies \quad \begin{bmatrix} 0 & a_2  \\ 1 & a_4 \end{bmatrix}\begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} a_2 \\ a_4 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ thus $a_2 = 1, a_4 = 0$ so $A = \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}$
\end{solution}



\subsection{Exercise} \textbf{(Matrix representation for operator products)} Suppose $A$ is a linear operator from vector space $V$ to vector space $W$ , and $B$ is a linear operator from vector space $W$ to vector space $X$. Let $\ket{v_i}, \ket{w_j}$, and $ \ket{x_k}$ be bases for the vector spaces $V,W$, and $X$, respectively. Show that the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.

\begin{solution}
    With linear operators $A: V \mapsto W$, $B: W \mapsto X$ with basis $\ket{v_i} \in V, \quad \ket{w_j} \in W$, and $ \ket{x_k}\in X$ start with $BA\ket{v_i}$ then by associativity this equals
    $$B(A\ket{v_i}) = B \sum_j A_{ji}\ket{w_j} =  \sum_j A_{ji}(B\ket{w_j}) = \sum_k \sum_j B_{kj}A_{ji}\ket{x_k}$$ noting that $B_{kj}$ and $A_{ji}$ can be reordered since sums over finite indices are commutative and associative and $(BA)_{ki} $ has entries given by $\sum_j B_{kj} A_{ji}$.
\end{solution}




\subsection{Exercise}
\textbf{(Matrix representation for identity)} Show that the identity operator on a vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases. This matrix is known as the identity matrix.

\begin{solution}
    Let $I: V \mapsto V$ s.t. $I\ket{v} = \ket{v}$, $ \forall\ket{v} \in V$ and the basis for $V$ is $\ket{v_i}$ for $i \in \{1,...,n\}$ then each entry in $I$ is generated by 
    $$ I \ket{v_i} = \sum^n_j I_{ji}\ket{v_j} = \ket{v_i}$$
    thus $I_{ji} = 1 \iff j=i$ and 0 otherwise. Thus the matrix is $$I = \begin{bmatrix}
        1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1
    \end{bmatrix}$$
\end{solution}




\subsection{Exercise} Verify that $(\cdot, \cdot)$ just defined is an inner product on $\C^n$.

\begin{solution}
    Running the checklist for inner products find \begin{enumerate}[1)]
        \item Linearity (in 2nd argument): $$(\ket{y}, \sum_i \lambda_i\ket{z_i}) = \sum_j y_j^*\sum_i \lambda_i z_{ij} = \sum_i \lambda_i\sum_j  y_j^* z_{ij} = \sum_i \lambda_i(\ket{y},\ket{z_i})$$

        \item Conjugate Symmetry:
        $$(\ket{y}, \ket{z}) = \sum_i y_i^*z_i = \sum_i z_iy_i^* = \sum_i (z_i^*y_i)^* = (\ket{z}, \ket{y})^*$$

        \item Positive Definiteness: $$(\ket{y}, \ket{y}) = \sum_i y_i^*y_i =\sum_i |y_i|^2 $$ which is non-negative and note that $\sum|y_i|^2 = 0 \iff \forall i,  y_i = 0$
    \end{enumerate}
    Thus $(\cdot, \cdot)$ as defined is an inner product on $\C^n$.
\end{solution}




\subsection{Exercise} Show that any inner product $(\cdot, \cdot)$ is conjugate-linear in the first argument,

$$\bigg( \sum_i \lambda_i\ket{w_i},\ket{v}\bigg) = \sum_i \lambda_i^*\big(\ket{w_i}, \ket{v}\big)$$

\begin{solution}
    Using the conjugate symmetry property of the  inner product then linearity then conjugate symmetry again  find:
    $$ \bigg( \sum_i \lambda_i\ket{w_i},\ket{v}\bigg)  =  \bigg(\ket{v}, \sum_i \lambda_i\ket{w_i}\bigg)^*  = \sum_i \lambda_i^*\big(\ket{v}, \ket{w_i}\big)^*  = \sum_i \lambda_i^*\big(\ket{w_i}, \ket{v}\big)$$
\end{solution}




\subsection{Exercise} Verify that $\ket{w}\equiv(1,1)$ and $\ket{v}\equiv (1,-1)$ are orthogonal. What are the normalized forms of these vectors?

\begin{solution}
    To verify compute the inner product $$\braket{w}{v}  = \begin{bmatrix} 1 & 1 \end{bmatrix} 
    \begin{bmatrix}  1 \\ -1 \end{bmatrix} = 1 - 1 = 0$$ so the vectors are orthogonal and to normalize take $\frac{\ket{w}}{\norm{\ket{w}}} $ where  $\norm{\ket{w}} = \sqrt{\braket{w}} = \sqrt{2}$ then $\frac{\ket{w}}{\norm{\ket{w}}} = \frac{1}{\sqrt{2}}(1,1)$ and $\norm{\ket{v}} = \sqrt{\braket{v}} = \sqrt{2}$ so $\frac{\ket{v}}{\norm{\ket{v}}} = \frac{1}{\sqrt{2}}(1,-1)$ thus $\frac{1}{\sqrt{2}}(1,1), \quad \frac{1}{\sqrt{2}}(1,-1)$ are the normalized forms of $\ket{w}$ and $\ket{v}$ respectively.
\end{solution}




\subsection{Exercise} Prove that the Gram-Schmidt procedure produces an orthonormal basis for $V$. 

\begin{solution}
Starting with $\ket{w_1},...,\ket{w_d}$ as a basis for vector space $V$  consider for $1\leq k \leq d - 1$ a subspace $V_k =$ Span$\{\ket{w_1},...,\ket{w_k}\}$.

Now to construct an orthogonal basis set $\ket{u_1} = \ket{w_1}$ so Span$\{\ket{u_1}\}=$ Span$\{\ket{w_1}\}$ continuing inductively for some $k < d-1$ an orthogonal basis has been constructed by $\ket{u_1},...,\ket{u_k}$ by defining $\ket{u_{k+1}} = \ket{w_{k+1}}-\sum^k_{i=1}\braket{u_i}{w_{k+1}}\ket{u_i}$ for $V_k$ by removing each non-orthogonal component of all previous vectors already in the orthogonal basis, i.e. by the Orthogonal Decomposition Theorem $\ket{u_{k+1}}$ is orthogonal to $V_k$ and $\ket{w_{k+1}} \in V_{k+1} $ then since $V_{k+1}$ is closed under subtraction $\implies \ket{u_{k+1}} \in V_{k+1}$. 

Note that $\ket{u_{k+1}} \neq 0$ since $\ket{w_{k+1}} \notin V_k$ so $\ket{u_1},...,\ket{u_k+1}$ is an orthogonal set of vectors forming a basis for $V_{k+1}$. Halt the procedure when $V_k \equiv V$.

To make the set orthonormal set $\ket{v_1} = \ket{u_1}/\norm{\ket{u_1}}$ and normalize each vector $\ket{v_{k+1}} = \ket{u_{k+1}}/\norm{\ket{u_{k+1}}}$ which finishes the proof for Gram-Schmidt producing an orthonormal basis for $V$. By putting it all together results in 2.17 as  $$
\ket{v_{k+1}} \equiv \frac{\ket{w_{k+1}} - \sum^k_{i=1}\braket{v_i}{w_{k+1}}\ket{v_i}}{\norm{\ket{w_{k+1}} - \sum^k_{i=1}\braket{v_i}{w_{k+1}}\ket{v_i}}}$$ 
\end{solution}




\subsection{Exercise} 
\textbf{(Pauli operators and the outer product)} The Pauli matrices can be considered as operators with respect to an orthonormal basis $\ket{0}, \ket{1}$ for a two-dimensional Hilbert space. Express each of the Pauli operators in the outer product notation.

\begin{solution}
    $$\sigma_0 \equiv I \equiv 
    \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \equiv 
    \ket{0}\bra{0} + \ket{1}\bra{1}, \quad
    \sigma_1 \equiv X \equiv 
    \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \equiv 
    \ket{0}\bra{1} + \ket{1}\bra{0}
    $$ 
    $$
    \sigma_2 \equiv Y \equiv 
    \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} \equiv 
    i\ket{0}\bra{1} -i\ket{1}\bra{0}, \quad
    \sigma_3 \equiv Z \equiv 
    \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \equiv 
    \ket{0}\bra{0} - \ket{1}\bra{1}
    $$
    which completes the 4 Pauli matrices listed in 2.2.
\end{solution}




\subsection{Exercise} Suppose $\ket{v_i}$ is an orthonormal basis for an inner product space $V$. What is the matrix representation for the operator $\ket{v_j}\bra{v_k}$, with respect to the $\ket{v_i}$ basis?

\begin{solution}
    Applying the operator to the orthonormal basis find $\ket{v_j}\braket{v_k}{v_i}$ which equals 0 where $k \neq i$ and equals $\ket{v_j}$ where $k = i$ thus $\ket{v_j}\bra{v_k}$ is all zeroes except in row  $j$ and column $k$.
\end{solution}




\subsection{Exercise} \textbf{(Eigendecomposition of the Pauli matrices)} Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.

\begin{solution}
    For $X$'s eigenvalues and eigenvectors find:
    $$\det(X-I\lambda) = \lambda^2 -1 = 0 \implies \lambda = \pm 1, 
    \quad (X - I)\ket{v} = \textbf{0} \implies \ket{v_1} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, 
    (X+I)\ket{v} = \textbf{0} \implies \ket{v_2} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$$
    then the orthonormal set of eigenvectors is $\ket{v_1} \mapsto \ket{+} = \frac{1}{\sqrt{2}}\ket{v_1}$ and $\ket{v_2} \mapsto \ket{-}=\frac{1}{\sqrt{2}}\ket{v_2}$ so its diagonal representation is given by $X = \sum_i \lambda_i\ket{i}\bra{i} = (1)\ket{+}\bra{+} + (-1)\ket{-}\bra{-} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
    $ which is diagonal in the $\{\ket{+},\ket{-}\}$ basis.

    For $Y$'s eigenvalues and eigenvectors find:
    $$\det(Y-I\lambda) = \lambda^2 - 1 = 0 \implies \lambda = \pm 1, \quad 
    (Y - I)\ket{v} = \textbf{0} \implies \ket{v_1} = \begin{bmatrix} 1 \\ i \end{bmatrix}, 
    (Y+I)\ket{v} = \textbf{0} \implies \ket{v_2} = \begin{bmatrix} 1 \\ -i \end{bmatrix}
    $$
    again the orthonormal set of eigenvectors is $\ket{v_1} \mapsto \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ i \end{bmatrix}$ and $\ket{v_2} \mapsto \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -i \end{bmatrix}$ so its diagonal representation is given by $Y = \sum_i \lambda_i\ket{i}\bra{i} = (1)\ket{v_1}\bra{v_1} + (-1)\ket{v_2}\bra{v_2} = \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}
    $.

    For $Z$'s eigenvalues and eigenvectors find:
    $$\det(Z-I\lambda) = \lambda^2 - 1 = 0 \implies \lambda = \pm 1, \quad 
    (Z - I)\ket{v} = \textbf{0} \implies \ket{v_1} = \ket{0}, 
    (Z+I)\ket{v} = \textbf{0} \implies \ket{v_2} = \ket{1}
    $$
    where $\ket{0}, \ket{1}$ are already orthonormal then its diagonal representation $Z = \sum_i \lambda_i\ket{i}\bra{i} = (1)\ket{0}\bra{0} + (-1)\ket{1}\bra{1} = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
    $.
\end{solution}



\subsection{Exercise} Prove that the matrix $\begin{bmatrix}
    1 & 0 \\ 1 & 1
\end{bmatrix}$ is not diagonalizable. 

\begin{solution}
    An operator is diagonalizable if and only if it has a diagonal representation $A = \sum_i \lambda_i\ket{i}\bra{i}$ where each $\ket{i}$ form an eigenbasis. Now proceeding to the eigenvalues and eigenvectors of the given matrix are 
    $$\det\begin{bmatrix}
        1-\lambda & 0 \\ 1 & 1-\lambda
    \end{bmatrix} = (1-\lambda)^2= 0 \implies \lambda = 1 \text{ with eigenvector } \ket{1}$$ because the matrix only has one eigenvector a complete eigenbasis cannot be formed, for a $2\cross 2$ matrix two linear independent eigenvectors are needed. Thus the matrix is not diagonalizable. 
\end{solution}



\subsection{Exercise} If $\ket{w}$ and $\ket{v}$ are any two vectors, show that $(\ket{w}\bra{v})^\dagger = \ket{v}\bra{w}$.

\begin{solution}
    Apply the rule $(AB)^\dagger = B^\dagger A^\dagger$ to the given outer product $(\ket{w}\bra{v})^\dagger= (\bra{v})^\dagger(\ket{w})^\dagger = \ket{v}\bra{w}$ which was to be shown. 
\end{solution}



\subsection{Exercise} \textbf{(Anti-linearity of the adjoint)} Show that the adjoint operation is anti-linear,
$$
\bigg(\sum_ia_i A_i\bigg)^\dagger = \sum_i a^*_i A^\dagger_i$$

\begin{solution}
    Let $A =\sum_ia_i A_i $ then it follows that $A^\dagger = \big(\sum_ia_i A_i\big)^\dagger$ now apply 2.32 to find
    $$(A^\dagger\ket{v}, \ket{w}) = \big((\sum_ia_i A_i)^\dagger\ket{v}, \ket{w}\big)  = (\ket{v}, \sum_ia_i A_i\ket{w}) 
    \quad\quad\quad $$ $$ \quad\quad\quad = \sum_ia_i(\ket{v}, A_i\ket{w}) = \sum_ia_i(A_i^\dagger \ket{v}, \ket{w})  = (\sum_ia_i^*A_i^\dagger \ket{v}, \ket{w}) $$ noting that the left side ("bra" side) takes the complex conjugate of $a_i$. This is because the inner product is linear in the second argument but anti-linear in the first.
\end{solution}



\subsection{Exercise} Show that $(A^\dagger)^\dagger=A$.

\begin{solution}
    Apply 2.32 twice to find $(\ket{v}, (A^\dagger)^\dagger\ket{w}) = (A^\dagger\ket{v}, \ket{w}) = (\ket{v}, A\ket{w})$ thus $(A^\dagger)^\dagger=A$ because there exists a \textbf{unique} linear operator $A^\dagger$ on $V$. This uniqueness forces $(A^\dagger)^\dagger=A$.
\end{solution}



\subsection{Exercise} Show that any projector $P$ satisfies the equation $P^2 = P$

\begin{solution}
    By definition $P = \sum_{i=1}^k\ket{i}\bra{i}$ then $$P^2 = \sum_{i=1}^k\ket{i}\bra{i}\sum_{j=1}^k\ket{j}\bra{j}= \sum_{i=1}^k\sum_{j=1}^k\ket{i}\braket{i}{j}\bra{j} = 
    \sum_{i=1}^k\sum_{j=1}^k\ket{i}\delta_{ij}\bra{j} = \sum_{i=1}^k\ket{i}\bra{i} = P$$

    Noting that $\delta_{ij} = 0$ unless $i=j$.
    
\end{solution}



\subsection{Exercise} Show that a normal matrix is Hermitian if and only if it has real eigenvalues.

\begin{solution}

    If a matrix $A$ is normal and Hermitian(self-adjoint) then $AA^\dagger = A^\dagger A$ and $A = A^\dagger$ since the operator is normal by the spectral decomposition theorem it can be written as $A = \sum_i \lambda_i\ket{i}\bra{i}$ and $A^\dagger= \sum_i \lambda_i^*\ket{i}\bra{i}$ but because $A$ is self-adjoint $$\sum_i \lambda_i\ket{i}\bra{i}= \sum_i \lambda_i^*\ket{i}\bra{i} \implies \lambda = \lambda^*$$
    Then for $\lambda := a + bi \in \C$ the implication is $a + bi = a - bi \implies b = 0$ so the eigenvalues have no complex component thus they are real eigenvalues. $\square$

    Converse:

    If the matrix $A$ is normal and has real eigenvalues then by the spectral decomposition theorem it can be written as $A = \sum_i \lambda_i\ket{i}\bra{i}$ and with real eigenvalues $\lambda = \lambda^*$ so  $$A = \sum_i \lambda_i\ket{i}\bra{i} = \sum_i \lambda_i^*\ket{i}\bra{i} = A^\dagger$$
    which implies $A$ is Hermitian. $\square$
\end{solution}



\subsection{Exercise} Show that all eigenvalues of a unitary matrix have modulus 1, that is, can be written in the form $e^{i\theta}$ for some real $\theta$.

\begin{solution}
    For a given unitary matrix $U$ write $U = \sum_i\lambda_i\ket{w_i}\bra{v_i}$ for two orthonormal bases $\ket{w_i},\ket{v_i}$. Using the property $UU^\dagger = I$ then combining the above gives $$
    UU^\dagger = \bigg(\sum_i\lambda_i\ket{w_i}\bra{v_i}\bigg)\bigg(\sum_i\lambda_i\ket{w_i}\bra{v_i}\bigg)^\dagger = \sum_i\lambda_i\ket{w_i}\bra{v_i}\sum_i\lambda_i^*(\ket{w_i}\bra{v_i})^\dagger = \quad$$
    $$\quad \sum_i\lambda_i\ket{w_i}\bra{v_i}\sum_i\lambda_i^*\ket{v_i}\bra{w_i}=\sum_i\sum_i\lambda_i\lambda_i^*\ket{w_i}\braket{v_i}\bra{w_i} = \sum_i|\lambda_i|^2\ket{w_i}\bra{w_i} = I$$

    Noting that the second sum can be dropped because any time the indices of the two sums are different they produce 0. This also implies for each eigenvalue $|\lambda_i|^2 = 1 \implies |\lambda_i|=1$ so then it can be written as $\lambda_i\lambda^*_i  = e^{i\theta}e^{-i\theta}=e^0 = 1$ for some real $\theta$ because any complex number mod 1 can be written $e^{i\theta}$.
\end{solution}




\subsection{Exercise} \textbf{(Pauli matrices: Hermitian and unitary)} Show that the Pauli matrices are Hermitian and unitary. 

\begin{solution}
For matrices $I,X,Y$, and $Z$ find the following 
    $$II^\dagger = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix}\begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = I, \quad 
    XX^\dagger = \begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = I$$
    $$
    YY^\dagger = \begin{bmatrix}0 & -i \\ i & 0 \end{bmatrix}\begin{bmatrix}0 & -i \\ i & 0 \end{bmatrix}=\begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix}=I, \quad 
    ZZ^\dagger = \begin{bmatrix}1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix}1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix} = I$$

    Note that in all cases each matrix equals its adjoint (transpose then conjugate) and they all satisfy the unitary requirement.
\end{solution}



\subsection{Exercise} \textbf{(Basis changes)} 
Suppose $A^\prime$ and $A^{\prime\prime}$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\ket{v_i}$ and $\ket{w_i}$. Then the elements of $A^\prime$ and $A^{\prime\prime}$ are $A^\prime_{ij} = \bra{v_i}A\ket{v_j}$ and $A^{\prime\prime}_{ij} = \bra{w_i}A\ket{w_j}$. Characterize the relationship between $A^\prime$ and $A^{\prime\prime}$.

\begin{solution}
    Since $U$ is defined as the outer product between two orthonormal bases sets $U \equiv \sum_i\ket{w_i}\bra{v_i} $ $\implies U_{ij} = \braket{v_i}{w_j}$ (Note this is by choice as $U\equiv \sum_i\ket{v_i}\bra{w_i}$ is valid as long as the choice is consistent) then using this  $$
     A^\prime_{ij} = \bra{v_i}A\ket{v_j} = \bra{v_i}IAI\ket{v_j} = \bra{v_i}\sum_k\ket{w_k}\bra{w_k}A\sum_l\ket{w_l}\bra{w_l}\ket{v_j} = \sum_k\sum_l\bra{v_i}\ket{w_k}\bra{w_k}A\ket{w_l}\bra{w_l}\ket{v_j}$$
     $$
     = \sum_k\sum_l\bra{v_i}\ket{w_k}A^{\prime\prime}_{kl}\bra{w_l}\ket{v_j}= \sum_k\sum_lU_{ik}A^{\prime\prime}_{kl}\bra{v_j}\ket{w_l}^\dagger = \sum_k\sum_lU_{ik}A^{\prime\prime}_{kl}U_{jl}^\dagger$$
     More succinctly this can be written as $A^\prime = UA^{\prime\prime} U^\dagger$ which characterizes the matrix representations relationship. 
\end{solution}



\subsection{Exercise} Repeat the proof of the spectral decomposition in Box 2.2 for the case when $M$ is Hermitian, simplifying the proof wherever possible. 


\begin{solution}

    \textbf{Spectral Decomposition} - \textit{Any normal operator $M$ on a vector space $V$ is diagonal with respect to some orthonormal basis for $V$. Conversely, any diagonalizable operator is normal.}

    In this case $M$ is Hermitian so $M = M^\dagger$. 

    For $d= 1$, $M$ is obviously diagonal. Let $\lambda$ be an eigenvalue of $M$, $P$ a projector onto the $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal complement. Expanding $$M = IMI = (P+Q)M(P+Q) = (PM +QM)(P+Q)=PMP + QMP + PMQ + QMQ$$
    Where $QMP = PMQ = 0$, $\quad PMP = \lambda P$, and $QMQ$ is Hermitian then $QMQQM^\dagger Q=QM^\dagger QQM Q$ since $M = M^\dagger$ which gives
    $$M = \lambda P  +QMQ$$

    Then since $QMQ$ is diagonal with respect to an orthonormal basis for subspace $Q$ and $\lambda P$ is already diagonal then the sum is diagonal with respect to some orthonormal basis for $V$. Thus $M$ can be written as $\sum_i \lambda_i \ket{i}\bra{i}$ which was to be shown. $\square$

    For the converse, since $M$ is Hermitian it is normal. 
\end{solution}



\subsection{Exercise} Prove that two eigenvectors of a Hermitian operator with different eigenvalues are necessarily orthogonal.

\begin{solution}
    Let $\ket{v_1}$ and $\ket{v_2}$ be two eigenvectors of a Hermitian operator $M$ corresponding to different eigenvalues then by definition
    $$
    M\ket{v_1} = \lambda_1\ket{v_1} \quad \text{ and } \quad M\ket{v_2} = \lambda_2\ket{v_2}$$
    since $M$ is Hermitian by the spectral decomposition theorem $M = \sum_i \lambda_i \ket{i}\bra{i}$ then for two different eigenvalues $\lambda_1$ and $\lambda_2$, $\ket{i}\bra{i}\ket{v_1} = \ket{v_1} \iff \ket{i}=\ket{v_1}$ since eigenvectors only live in one projectors range thus for two different eigenvalues $$\bra{v_1}\ket{i}\bra{i}\ket{j}\bra{j}\ket{v_2}= 0 \iff i \neq j$$ so $\ket{v_1}$ and $\ket{v_2}$ must be orthogonal. $\square$
    
\end{solution}




\subsection{Exercise} Show that the eigenvalues of a projector $P$ are all either 0 or 1.

\begin{solution}
    First note that for a diagonal matrix the eigenvalues are just the entries along the diagonal of the matrix. Since $P \equiv \sum^k_{i=1} \ket{i}\bra{i}$ it is already diagonal and is essentially $I$ in a $k$-dimensional subspace. Denoting the full vector space as $d$-dimension, then $P$'s characteristic polynomial is  $\det(P - \lambda I ) = (1-\lambda)^k(-\lambda^{d-k}) = 0 $ thus $\lambda =$ 0 or 1. 
\end{solution}



\subsection{Exercise} \textbf{(Hermiticity of positive operators)} Show that a positive operator is necessarily Hermitian. (\textit{Hint:} Show that an arbitrary operator $A$ can be written $A=B+iC$ where $B$ and $C$ are Hermitian.)

\begin{solution}
    A positive operator $A$ is defined that for any vector $\ket{v}$, $\quad \bra{v}A\ket{v} \in \R_{\geq0}$. Following the hint split $A$, into $A = A/2 + A/2$ and add 0 to get $$A = \frac{A}{2} + \frac{A}{2} + \frac{A^\dagger}{2} - \frac{A^\dagger}{2} = \frac{A}{2} + \frac{A^\dagger}{2} + \frac{i}{i}\bigg(\frac{A}{2} - \frac{A^\dagger}{2}\bigg) \implies \quad B = \frac{A+A^\dagger}{2}, \quad C = \frac{A-A^\dagger}{2i}$$
    So $A = B + iC$ then using the linearity in the second argument of the inner product gives $$\bra{v}A\ket{v} = \bra{v}B + iC\ket{v} = \bra{v}B\ket{v} + \bra{v}iC\ket{v} \implies \quad C = \textbf{0}$$ with the implication coming from the fact that the result of the inner product is real. Now checking if $B$ is Hermitian to find
    $$
    B^\dagger = \bigg(\frac{A+A^\dagger}{2}\bigg)^\dagger = \frac{A^\dagger+A}{2}= \frac{A+A^\dagger}{2} = B$$ thus $B$ is Hermitian and $A = B $ therefore $A$ is Hermitian. $\square$
    
\end{solution}




\subsection{Exercise} Show that for any operator $A, \quad A^\dagger A $ is positive.

\begin{solution}
    Consider the following 
    $$\bra{v} A^\dagger A \ket{v}= \braket{Av}{Av}  = \norm{A\ket{v}}^2 \geq 0$$
    where the squared norm is always real and non-negative.
\end{solution}



\subsection{Exercise} Let $\ket{\psi}= (\ket{0}+\ket{1})/\sqrt{2}$. Write out $\ket{\psi}^{\otimes2}$ and $\ket{\psi}^{\otimes3}$ explicitly, both in terms of tensor products like $\ket{0}\ket{1}$, and using the Kronecker product. 

\begin{solution}
    $$\ket{\psi}^{\otimes2} = \frac{(\ket{0}+\ket{1})}{\sqrt{2}}\otimes \frac{(\ket{0}+\ket{1})}{\sqrt{2}} = \frac{1}{2}\big[ (\ket{0}+\ket{1})\ket{0} + (\ket{0}+\ket{1})\ket{1}  \big]= \frac{1}{2}\big[ \ket{0}\ket{0}+\ket{1}\ket{0} + \ket{0}\ket{1}+\ket{1}\ket{1}  \big]$$

    Continuing to $\ket{\psi}^{\otimes 3}$ as $\ket{\psi}^{\otimes2}\ket{\psi} $ find
    $$
    \ket{\psi}^{\otimes 3} = \frac{1}{2}\big[ \ket{0}\ket{0}+\ket{1}\ket{0} + \ket{0}\ket{1}+\ket{1}\ket{1}  \big]\otimes\frac{(\ket{0}+\ket{1})}{\sqrt{2}} 
    = \frac{1}{\sqrt{8}}\big[\ket{0}\ket{0}\ket{0} + \ket{0}\ket{0}\ket{1} + \ket{0}\ket{1}\ket{0} + ... + \ket{1}\ket{1}\ket{1}]
    $$
    Where counting in binary fills in the ellipsis (the "...") above. Onto the Kronecker product 
    $$\ket{\psi}^{\otimes2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} \otimes 
     \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2}\begin{bmatrix}
         1 \\ 1 \\ 1 \\ 1
     \end{bmatrix}, \quad  \ket{\psi}^{\otimes 3} = \frac{1}{2}\begin{bmatrix}
         1 \\ 1 \\ 1 \\ 1
     \end{bmatrix} \otimes \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{\sqrt{8}} \begin{bmatrix}
         1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1
     \end{bmatrix}
    $$
\end{solution}




\subsection{Exercise} Calculate the matrix representation of the Pauli operators (a) $X$ and $Z$; (b) $I$ and $X$; (c) $X$ and $I$. Is the tensor product commutative?

\begin{solution}
    Recall $$I = \begin{bmatrix} 1 & 0 \\ 0&1  \end{bmatrix}, \quad X = \begin{bmatrix} 0&1 \\ 1 &0\end{bmatrix}, \quad Z = \begin{bmatrix} 1&0 \\ 0&-1 \end{bmatrix}   $$ then the following are their tensor products 
    \begin{enumerate}
        \item $$X \otimes Z = \begin{bmatrix} \textbf{0} & Z \\ Z & \textbf{0} \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & -1  \\ 1 & 0 & 0 & 0 \\0 & -1 & 0 & 0 \end{bmatrix} $$
        \item $$I \otimes X = \begin{bmatrix} X & \textbf{0}  \\  \textbf{0} & X\end{bmatrix} = \begin{bmatrix} 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0  \\ 0 & 0 & 0 & 1 \\0 & 0 & 1 & 0 \end{bmatrix} $$
        \item $$X \otimes I = \begin{bmatrix} \textbf{0} & I \\ I & \textbf{0} \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1  \\ 1 & 0 & 0 & 0 \\0 & 1 & 0 & 0 \end{bmatrix} $$
    \end{enumerate}

    Note that case (b) and case (c), $X\otimes I \neq I \otimes X$ do not match which provides an example that the tensor product is not commutative. 
\end{solution}



\subsection{Exercise} Show that the transpose, complex conjugation, and adjoint operations distribute over the tensor product, $$
(A \otimes B)^*= A^* \otimes B^*; \quad
(A\otimes B)^T = A^T\otimes B^T; \quad
(A \otimes B)^\dagger = A^\dagger \otimes B^\dagger$$

\begin{solution}
    Expanding out into the matrix form find 
    $$
    (A\otimes B)^*  = \begin{bmatrix}
        a_{11}B & \cdots & a_{1n}B \\
        \vdots &  \ddots & \vdots  \\
        a_{m1}B & \cdots  & a_{mn}B
    \end{bmatrix}^* 
    = \begin{bmatrix}
        (a_{11}B)^* & \cdots & (a_{1n}B)^* \\
        \vdots &  \ddots & \vdots  \\
        (a_{m1}B)^* & \cdots  & (a_{mn}B)^*
    \end{bmatrix} 
    = \begin{bmatrix}
        a_{11}^*B^* & \cdots & a_{1n}^*B^* \\
        \vdots &  \ddots & \vdots  \\
        a_{m1}^*B^* & \cdots  & a_{mn}^*B^*
    \end{bmatrix} =  A^* \otimes B^*$$

    $$(A\otimes B)^T 
    = \begin{bmatrix}
        a_{11}b_{11} & \cdots & a_{1n}b_{1q} \\
        \vdots &  \ddots & \vdots  \\
        a_{m1}b_{p1} & \cdots  & a_{mn}b_{pq}
    \end{bmatrix}^T 
    = \begin{bmatrix}
        a_{11}b_{11} & \cdots &  a_{m1}b_{p1}\\
        \vdots &  \ddots & \vdots  \\
        a_{1n}b_{1q} & \cdots  & a_{mn}b_{pq}
    \end{bmatrix}
    = \begin{bmatrix}
        a_{11}B^T & \cdots &  a_{m1}B^T\\
        \vdots &  \ddots & \vdots  \\
        a_{1n}B^T & \cdots  & a_{mn}B^T
    \end{bmatrix} =  A^T \otimes B^T$$

    Combine the above two distributive properties to find the adjoint operation distributes: 

    $\quad (A \otimes B)^\dagger = [(A \otimes B)^T]^*
    = [A^T \otimes B^T]^* = A^{T*} \otimes B^{T*}
    = A^\dagger \otimes B^\dagger$
\end{solution}



\subsection{Exercise} Show the tensor product of two unitary operators is unitary.

\begin{solution}
    Let matrices $U_{m\cross m}$ and $W_{n\cross n}$ be \textit{unitary} then $UU^\dagger = I$ and $WW^\dagger=I$. Their tensor product $(U \otimes W)$ is unitary if $(U \otimes W)(U \otimes W)^\dagger = I_{mn\cross mn}$ with the identity matrix being the larger size so using 2.28 and equation (2.45) find
    $$(U \otimes W)(U \otimes W)^\dagger=(U \otimes W)(U^\dagger \otimes W^\dagger)=UU^\dagger \otimes WW^\dagger= I_{m\cross m}\otimes I_{n\cross n}= I_{mn\cross mn}$$

    which was to be shown. $\square$
\end{solution}



\subsection{Exercise} Show that the tensor product of two Hermitian operators is Hermitian. 

\begin{solution}
     Let matrices $A_{m\cross m}$ and $B_{n\cross n}$ be \textit{Hermitian} then $A = A^\dagger$ and $B = B^\dagger$. Their tensor product $(A \otimes B)$ is Hermitian if $(A \otimes B) = (A \otimes B)^\dagger$ so using 2.28 find 
     $$
     (A \otimes B) = (A^\dagger \otimes B^\dagger) = (A \otimes B)^\dagger$$
     which was to be shown. $\square$
\end{solution}



\subsection{Exercise} Show that the tensor product of two positive operators is positive.

\begin{solution}
    Let matrices $A$ and $B$ be two positive operators on $V$ and $W$ respectively, then for $\ket{v} \in V$, $\bra{v}A\ket{v}\geq 0$ and for $\ket{w} \in W$, $\bra{w}B\ket{w}\geq 0$. Their tensor product is positive if $(A \otimes B )$ has $\bra{v}A\ket{v}\bra{w}B\ket{w} \geq 0$ so by equation (2.49)
    $$
    \bigg(\ket{v}\otimes\ket{w}, (A \otimes B )(\ket{v}\otimes\ket{w})\bigg)= \bigg(\ket{v}\otimes\ket{w}, A\ket{v}\otimes B\ket{w}\bigg) = \bra{v}A\ket{v}\bra{w}B\ket{w} \geq 0$$ since both $\bra{v}A\ket{v}$ and $\bra{w}B\ket{w}$ are greater or equal to zero so is there product which was to be shown. $\square$
\end{solution}




\subsection{Exercise} Show that the tensor product of two projectors is a projector.

\begin{solution}
    Let projectors $P$ and $R$ be projectors on $W_k \subset V_m$ and $U_l \subset Y_n$ respectively, with the dimensional size in the subscripts.  $P \otimes R$ is a projector on $W \otimes U$ if $P \otimes R \equiv \sum_i^{kl}\ket{i}\bra{i}$ so 
    $$
    P \otimes R = \sum_i^k\ket{i}\bra{i}\otimes\sum_j^l\ket{j}\bra{j }=\sum_i^k\sum_j^l\ket{i\otimes j}\bra{i\otimes j} = \sum_i^{kl}\ket{i}\bra{i}$$
    since for all $i \neq j$ the matrix entry is 0. Thus the tensor product of two projectors is a projector which was to be shown. 
\end{solution}




\subsection{Exercise} The Hadamard operator on one qubit may be written as $$
H = \frac{1}{\sqrt{2}} \big[(\ket{0}+ \ket{1})\bra{0} + (\ket{0}-\ket{1})\bra{1}\big]$$ 
Show explicitly that the Hadamard transform on $n$ qubits, $H^{\otimes n}$ may be written as 
$$
H^{\otimes n} = \frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\ket{x}\bra{y}$$
Write out an explicit matrix representation for $H^{\otimes 2}$

\begin{solution}
    Writing $\ket{x}\otimes \ket{y}$ as $\ket{xy}$ and starting with $H^{\otimes 2}$ find $$
    H\otimes H = \frac{1}{\sqrt{2}} \big[(\ket{0}+ \ket{1})\bra{0} + (\ket{0}-\ket{1})\bra{1}\big] \otimes \frac{1}{\sqrt{2}} \big[(\ket{0}+ \ket{1})\bra{0} + (\ket{0}-\ket{1})\bra{1}\big] \quad $$
    $$= \frac{1}{\sqrt{4}}\bigg[ \big((\ket{0}+ \ket{1})\bra{0}\big)^{\otimes2} + (\ket{0}-\ket{1})\bra{1}\otimes(\ket{0}+ \ket{1})\bra{0} + (\ket{0}+\ket{1})\bra{0}\otimes(\ket{0}- \ket{1})\bra{1} + \big((\ket{0}-\ket{1})\bra{1}\big)^{\otimes2}\bigg]
    $$ $$
    = \frac{1}{\sqrt{4}}\bigg[ (\ket{00}+ \ket{10}+\ket{01} +\ket{11})\bra{00} +
    (\ket{00}-\ket{10} +\ket{01}-\ket{11})\bra{10} +
    \hspace{100pt} $$ $$ \hspace{100pt}
    (\ket{00}+\ket{10} -\ket{01}-\ket{11})\bra{01} +
    (\ket{00}-\ket{10} -\ket{01}+\ket{11})\bra{11} \bigg]$$
    $$= \frac{1}{\sqrt{4}}\begin{bmatrix} 
        1 & 1 & 1 & 1 \\ 
        1 & -1 & 1 & -1  \\ 
        1 & 1 & -1 & -1 \\
        1 & -1 & -1 & 1 \end{bmatrix} = H^{\otimes 2} $$
    \textbf{Proof for $H^{\otimes n}$ : }Now that that's out of the way, its time to show the general $H^{\otimes n}$ form is 
    $
    H^{\otimes n} = \frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\ket{x}\bra{y}$ starting with the basic $n=1$ case which is straight forward 
    $$
    H^{\otimes 1} = \frac{1}{\sqrt{2}} \big[(\ket{0}+ \ket{1})\bra{0} + (\ket{0}-\ket{1})\bra{1}\big] = \frac{1}{\sqrt{2}} \sum_{y=0}^1(\ket{0}+ (-1)^y\ket{1})\bra{y}
    =\frac{1}{\sqrt{2^1}} \sum_{y=0}^1\sum_{x=0}^1(-1)^{x\cdot y}\ket{x}\bra{y}$$
    Now to consider the generalized $H^{\otimes n}$ holds for an arbitrary $n$ and noting that $H$ from above can be written as $$H = \frac{1}{\sqrt{2^1}} \sum_{y=0}^1\sum_{x=0}^1(-1)^{x\cdot y}\ket{x}\bra{y} = \frac{1}{\sqrt{2}} \sum_{x,y\in\{0,1\}}(-1)^{x\cdot y}\ket{x}\bra{y}$$
    and expanding a general $H^{\otimes n}$ then factoring the $1/\sqrt{2}$ forward gives
    $$
    H^{\otimes n} = \frac{1}{\sqrt{2}} \sum_{x_1,y_1\in\{0,1\}}(-1)^{x_1\cdot y_1}\ket{x_1}\bra{y_1} \otimes ... \otimes \frac{1}{\sqrt{2}} \sum_{x_n,y_n\in\{0,1\}}(-1)^{x_n\cdot y_n}\ket{x_n}\bra{y_n} 
    \quad $$ $$ \quad
    = \frac{1}{\sqrt{2^n}} \bigg[\sum_{x_1,y_1\in\{0,1\}}(-1)^{x_1\cdot y_1}\ket{x_1}\bra{y_1} \otimes ... \otimes  \sum_{x_n,y_n\in\{0,1\}}(-1)^{x_n\cdot y_n}\ket{x_n}\bra{y_n}\bigg]
    $$ $$ \quad\quad
    = \frac{1}{\sqrt{2^n}} \bigg[\sum_{x_1,y_1\in\{0,1\}}...\sum_{x_n,y_n\in\{0,1\}}(-1)^{x_1\cdot y_1}\ket{x_1}\bra{y_1} \otimes ... \otimes  (-1)^{x_n\cdot y_n}\ket{x_n}\bra{y_n}\bigg]
    $$ 
    after pushing all the sums forwards and factoring the $(-1)$ terms forward, we have (2.55) $$ 
    = \frac{1}{\sqrt{2^n}} \bigg[\sum_{x_1,y_1\in\{0,1\}}...\sum_{x_n,y_n\in\{0,1\}}(-1)^{x_1\cdot y_1}...(-1)^{x_n\cdot y_n}\big(\ket{x_1}\bra{y_1} \otimes ... \otimes  \ket{x_n}\bra{y_n}\big)\bigg]
    $$ $$
    = \frac{1}{\sqrt{2^n}}\sum_{x,y\in\{0,1\}^n}(-1)^{x_1\cdot y_1+...+x_n\cdot y_n}\big(\ket{x_1...x_n}\bra{y_1...y_n} \big) =  \frac{1}{\sqrt{2^n}}\sum_{\textbf{x,y}}(-1)^{\textbf{x}\cdot\textbf{ y}}\ket{\textbf{x}}\bra{\textbf{y}}$$
    which was to be shown. $\square$

    It's important to note that the final sum is summing over all bit strings of length $n$ which is slightly obfuscated in the original problem.
\end{solution}



\subsection{Exercise} Find the square root and logarithm of the matrix $$\begin{bmatrix}
    4 & 3 \\ 3 & 4
\end{bmatrix}$$

\begin{solution}
    Let $M$ be the above matrix, to find the listed functions $M$ must be in its spectral decomposition form which is comprised of its eigenvalues with their corresponding eigenvectors computing 
    $$\det (M-I\lambda) = (4-\lambda)^2 - 9 = \lambda^2 -8\lambda + 7 = (\lambda -7)(\lambda-1) = 0 \implies \quad
    \lambda = 1, 7$$
    and their corresponding eigenvectors are given by 
    $$
    \begin{bmatrix}
    3 & 3 \\ 3 & 3
    \end{bmatrix}\begin{bmatrix}
        1 \\ -1
    \end{bmatrix} = \textbf{0} 
    \quad \text{ and } \quad 
    \begin{bmatrix}
    -3 & 3 \\ 3 & -3
    \end{bmatrix}\begin{bmatrix}
        1 \\ 1
    \end{bmatrix} = \textbf{0} \implies  \quad \ket{v_1}=\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 \\ -1
    \end{bmatrix}, \quad \ket{v_2}=\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 \\ 1
    \end{bmatrix}
    $$
    Now to use $f(M) \equiv \sum_i f(\lambda_i) \ket{i}\bra{i}$ which assumes an orthonormal eigenbasis, hence why the eigenvectors were normalized.
    
    
    \textbf{Square Root:} $f = \sqrt{m}$ so for matrix $M$ find 
    $$
    f(M) = \sum_{i=1}^2\sqrt{\lambda_i}\ket{i}\bra{i}=
    1\ket{v_1}\bra{v_1} + \sqrt{7}\ket{v_2}\bra{v_2} = 
    \frac{1}{2}\begin{bmatrix}
        1 & -1 \\ -1 & 1
    \end{bmatrix} + \frac{\sqrt{7}}{2}\begin{bmatrix}
        1 & 1 \\ 1 & 1
    \end{bmatrix} = \begin{bmatrix}
        \frac{\sqrt{7} + 1}{2} & \frac{\sqrt{7} - 1}{2} \\
        \frac{\sqrt{7} - 1}{2} & \frac{\sqrt{7} + 1}{2}
    \end{bmatrix}$$

    \textbf{Logarithm} Recall this book refers to base 2 logarithms unless otherwise noted so $f = \log_2(m)$ then 
    $$
    f(M) = \sum_{i=1}^2\log(\lambda_i)\ket{i}\bra{i}= \log(7)\ket{v_2}\bra{v_2}=\frac{\log(7)}{2}\begin{bmatrix}
        1 & 1 \\ 1 & 1
    \end{bmatrix}$$
    Note that $\log(1) = 0$. That concludes the results of both functions.
\end{solution}




\subsection{Exercise} \textbf{(Exponential of the Pauli Matrices)} Let $\vec{v}$ be any real, three-dimensional unit vector and $\theta$ a real number. Prove that 
$$\exp(i\theta \vec{v}\cdot \vec{\sigma}) = \cos(\theta)I + i\sin(\theta)\vec{v}\cdot\vec{\sigma}$$
where $\vec{v}\cdot \vec{\sigma}\equiv \sum^3_{i=1} v_i\sigma_i$. This exercise is generalized in Problem 2.1 on page 117.

\begin{solution}
    Recall that $\sum^3_{i=1}\sigma_i = X+Y+Z$ and expand the sum $\sum^3_{i=1} v_i\sigma_i $ so the matrix is more clear 

    $$
    \sum^3_{i=1} v_i\sigma_i = v_1X + v_2Y +v_3Z = 
    \begin{bmatrix}
        0 & v_1 \\ v_1 & 0
    \end{bmatrix} + \begin{bmatrix}
        v_2 & 0 \\ 0 & -v_2 
    \end{bmatrix} + \begin{bmatrix}
        0 & -iv_3 \\ iv_3 & 0
    \end{bmatrix} = \begin{bmatrix}
        v_2 & v_1 -iv_3 \\ v_1 + iv_3 & - v_2
    \end{bmatrix}$$
    Now note that $\vec{v}$ is a real, 3D unit vector which $\implies \lambda = \pm 1$ also note that $\begin{bmatrix}
        v_2 & v_1 -iv_3 \\ v_1 + iv_3 & - v_2
    \end{bmatrix} = \begin{bmatrix}
        v_2 & v_1 -iv_3 \\ v_1 + iv_3 & - v_2
    \end{bmatrix}^\dagger$ so the matrix is Hermitian so it has a spectral decomposition but for fun lets find the corresponding eigenvectors  
    $$
    \begin{bmatrix}
        v_2 - 1 & v_1 -iv_3 \\ v_1 + iv_3 & - v_2-1
    \end{bmatrix} \begin{bmatrix}
        1 + v_2 \\ v_1 + iv_3
    \end{bmatrix}=\begin{bmatrix}
        0 \\ 0
    \end{bmatrix}, \quad \begin{bmatrix}
        v_2 + 1 & v_1 -iv_3 \\ v_1 + iv_3 & - v_2+1
    \end{bmatrix} \begin{bmatrix}
        1 - v_2 \\ -v_1 - iv_3
    \end{bmatrix}=\begin{bmatrix}
        0 \\ 0
    \end{bmatrix}
    $$
    which works again since $\vec{v}$ is a unit vector then normalizing each vector $\ket{e_1},\ket{e_2}$ with $1/\sqrt{2(1+v_2)}$ and $1/\sqrt{2(1-v_2)}$ respectively. When $v_2 = \pm1$ no normalization is needed factor is needed. 
    
    \begin{equation*}
        \begin{split}
         \exp(i\theta \vec{v} \cdot \vec{\sigma}) &= \exp(i\theta 1)\ket{e_1}\bra{e_1} +  \exp(i\theta (-1))\ket{e_2}\bra{e_2} \\
         &= \big(\cos(\theta) + i\sin (\theta)\big)\ket{e_1}\bra{e_1} + \big(\cos(\theta) - i\sin (\theta)\big)\ket{e_2}\bra{e_2} \\
         &= \cos(\theta)\big(\ket{e_1}\bra{e_1} + \ket{e_2}\bra{e_2}\big) + i\sin(\theta)\big(\ket{e_1}\bra{e_1} - \ket{e_2}\bra{e_2}\big) \\
         &= \cos(\theta)I + i\sin(\theta)\vec{v}\cdot\vec{\sigma}
        \end{split}
        \end{equation*}
        The last equivalence is given by the fact that the outer products of orthonormal eigenvectors are essentially projectors. Further they are orthogonal projectors on to the eigenspace thus their sum is the identity and their difference is the original matrix $\vec{v}\cdot\vec{\sigma}$ which completes the proof. $\square$
        
    
\end{solution}



\subsection{Exercise} Show that the Pauli matrices except for $I$ have trace zero.

\begin{solution}
    $$X = \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix} \implies \quad \tr(X) = 0 + 0 = 0$$
    $$Y = \begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix} \implies  \quad \tr(Y) = 0 + 0 = 0$$
    $$Z = \begin{bmatrix}
        1 & 0\\ 0 & -1
    \end{bmatrix}\implies  \quad \tr(Z) = 1 -1 = 0$$
\end{solution}





\subsection{Exercise} \textbf{(Cyclic property of the trace)} If $A$ and $B$ are two linear operators show that $$
\tr(AB) = \tr(BA)$$

\begin{solution}
    For $A_{m\cross n}$ and $B_{n\cross m}$ observe that
    $$\tr(AB) = \sum_i (AB)_{ii} = \sum^m_i\sum^n_jA_{ij}B_{ji} = \sum^n_j\sum^m_iB_{ji}A_{ij}=\sum_i (BA)_{ii} = \tr(BA)$$

    where for square matrices the cyclic property emerges as $\tr(A_1A_2A_3) = \tr(A_3A_1A_2)$
\end{solution}




\subsection{Exercise} \textbf{(Linearity of the trace)} If $A$ and $B$ are two linear operators, show that 
$$\tr(A + B) = \tr(A) + \tr(B)$$ and if $z$ is an arbitrary complex number show that $$\tr(zA) = z\tr(A)$$

\begin{solution}
    If $A$ and $B$ are two linear operators then 
    $$\tr(A + B) = \sum_i(A+ B)_{ii} = \sum_i (A_{ii} + B_{ii}) = \sum_i A_{ii} + \sum_i B_{ii}  = \tr(A) + \tr(B)$$
    since during $A + B$ the diagonal entries are directly summed and the trace is a sum over those entries the order of operations doesn't matter.

    If $z$ is an arbitrary complex number then 
    $$\tr(zA) = \sum_i(zA)_{ii} = z\sum_iA_{ii} = z\tr(A)$$
    which was to be shown.
    
\end{solution}


\subsection{Exercise} \textbf{(The Hilbert-Schmidt inner product on operators)} The set $L_V$ of linear operators on a Hilbert space $V$ is obviously a vector space - the sum of two linear operators is a linear operator, $zA$ is a linear operator if $A$ is a linear operator and $z$ is a complex number, and there is zero element 0. An important additional result is that the vector space $L_V$ can be given a natural inner product structure, turning it into a Hilbert space.
\begin{enumerate}[(1)]
    \item Show that the function $(\cdot,\cdot)$ on $L_V\cross L_V$ defined by $(A,B) \equiv \tr(A^\dagger B)$ is an inner product function. This inner product is known as the \textit{Hilbert-Schmidt} or \textit{trace} inner product.
    \item If $V$ has $d$ dimensions show that $L_V$ has dimensions $d^2$.
    \item Find an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$
\end{enumerate}




\begin{solution}
    \begin{enumerate}[(1)]
        \item Recall that for $(A,B) \equiv \tr(A^\dagger B)$ to be an inner product it must satisfy the following three properties: 
        \begin{enumerate}
            \item \textbf{Linearity in the second argument:} Recall from 2.38 that the trace is linear then  
            $$
            (A,\sum_i\lambda_i B_i) = \tr(A^\dagger \sum_i\lambda_i B_i) = \tr(\sum_i\lambda_i A^\dagger B_i)=\sum_i\lambda_i \tr(A B_i) = \sum_i\lambda_i (A^\dagger, B_i)
            $$

            \item \textbf{Conjugate symmetry:} Recall the cyclic property of the trace from 2.37 then 
            $$
            (A,B) = \tr (A^\dagger B) = \tr(BA^\dagger) = \tr\big( (B^\dagger A)^\dagger) = \tr (B^\dagger A)^\dagger = (B, A)^\dagger
            $$

            \item \textbf{Positive-definiteness:} Note that for $A$ with entries $A_{ij}$, $A^\dagger = A^{T*}$ has entries $A_{ji}^*$ then 
            
            $$
            (A,A) = \tr(A^\dagger A ) = \tr(\sum_i^n\sum_j^n A_{ji}^* A_{ji} ) = \tr (\sum_i^n\sum_j^n |A_{ij}|^2) \geq 0$$ 
            and the inner product only equals 0 when all entries $|A_{ij}|^2$ are 0.
        \end{enumerate}

        \item If $V$ has $d$ dimensions and $L_V: V \mapsto V$ then $L_V$ must be $d \cross d$ to preserve the input and output spaces. Thus $L_V$ has dimension $d^2$. As a side note $L_V$ must be square to use the trace operation in the first part of this problem so $d^2$ is only natural.

        \item For $i,j \in \{\ket{1},...,\ket{d}\}$, let $E_{ij} = \ket{i}\bra{j}$ be a basis element of $L_V$. Now since $\dim L_V=d^2$ there are $d^2$ of these basis elements defined by $\{E_{ij}\},\forall i \forall j\in d$. Then entry $m,n\in E_{ij}$ is given by $\bra{m}E_{ij}\ket{n} = \braket{m}{i}\braket{j}{n} = \delta_{mi}\delta_{jn}$ and note $$(E_{ij},E_{lk})=\tr(E_{ij}^\dagger E_{lk})=\tr(E_{ji}^* E_{lk})=\delta_{jl}\delta_{ik}$$ which tells us that this basis is orthonormal. 

        That was the easy part but the basis that was just constructed is \textbf{NOT} Hermitian. Recall a Hermitian matrix is self-adjoint (i.e $A^\dagger = A$). Thankfully from this orthonormal a Hermitian basis can be constructed by letting taking a pair of basis elements $E_{ij}$ and $E_{ji}$ and defining a new basis as 
        $$
        X=\frac{E_{ij}+E_{ji}}{\sqrt{2}},\quad \text{and}\quad Y=\frac{E_{ij}-E_{ji}}{-i\sqrt{2}}$$

        Note that $X=X^\dagger$ by swapping the indices and commuting the addition in the numerator. $Y = Y^\dagger$ by the same process except that the sign on the denominator flips and the negative one can be moved to the numerator to achieve equality. Thus both $X$ and $Y$ are Hermitian and combine these with $Z=E_{ii}$ the diagonals to have are a normalized Hermitian basis. 

        Further note that $X,Y$, and $Z$ are orthogonal as $(X,Y) = (X,Z) = (Z,Y) = 0$ which completes the orthonormal basis of Hermitian Matrices for $L_V$.
    \end{enumerate}
\end{solution}




\subsection{Exercise} \textbf{(Commutation relation for the Pauli matrices)} Verify the commutations relations
$$
[X,Y] = 2iZ; \quad [Y,Z] = 2iX; \quad [Z,X] = 2iY$$
There is an elegant way of writing this using $\varepsilon_{jkl}$, the antisymmetric tensor on three indices, for which $\varepsilon_{jkl}=0$ except for $\varepsilon_{123}=\varepsilon_{231}=\varepsilon_{312}=1$, and $\varepsilon_{321}=\varepsilon_{213}=\varepsilon_{132}=-1$:
$$[\sigma_j,\sigma_k]=2i\sum^3_{l=1}\varepsilon_{jkl}\sigma_l$$

\begin{solution}
Verifying...
    $$
    [X,Y] = \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}-\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}\begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix} =\begin{bmatrix}
        i & 0 \\ 0 & -i
    \end{bmatrix}-\begin{bmatrix}
        -i & 0 \\ 0 & i
    \end{bmatrix}=2i\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}=2iZ$$
    $$
    [Y,Z] = \begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}-\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix} = \begin{bmatrix}
        0 & i \\ i & 0
    \end{bmatrix}-\begin{bmatrix}
        0 & -i \\ -i & 0
    \end{bmatrix} =2i\begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}=2iX
    $$
    $$
    [Z,X]=\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}\begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}-\begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}=\begin{bmatrix}
        0 & 1 \\ -1 & 0
    \end{bmatrix}-\begin{bmatrix}
        0 & -1 \\ 1 & 0
    \end{bmatrix}=\frac{-2}{i}\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}=2iY$$
    For the last steps of $[Z,X]$, multiply by $\frac{i}{i}$ and note that $i =-\frac{1}{i}$.
\end{solution}




\subsection{Exercise} \textbf{(Anti-commutation relations for the Pauli matrices)} Verify the anti-commutation relations $$\{\sigma_i,\sigma_j\}=0$$ where $i\neq j$ are both chosen from the set 1, 2, 3. Also verify that $(i=0,1,2,3)$ $$\sigma_i^2=I$$


\begin{solution}
    Since $\{\sigma_i,\sigma_j\} = \sigma_i\sigma_j + \sigma_j\sigma_i =  \sigma_j\sigma_i +\sigma_i\sigma_j = \{\sigma_j,\sigma_i\}$ only half need to be calculated explicitly:
    $$\{\sigma_1,\sigma_2\}= \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix} + \begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}\begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix} =\begin{bmatrix}
        i & 0 \\ 0 & -i
    \end{bmatrix}+\begin{bmatrix}
        -i & 0 \\ 0 & i
    \end{bmatrix} = \textbf{0}
    $$ $$
    \{\sigma_2,\sigma_3\}=\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}+\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix} = \begin{bmatrix}
        0 & i \\ i & 0
    \end{bmatrix}+\begin{bmatrix}
        0 & -i \\ -i & 0
    \end{bmatrix} = \textbf{0}
    $$ $$
    \{\sigma_3,\sigma_1\} =\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}\begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix} + \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}=\begin{bmatrix}
        0 & 1 \\ -1 & 0
    \end{bmatrix}+\begin{bmatrix}
        0 & -1 \\ 1 & 0
    \end{bmatrix}= \textbf{0}
    $$
    which completes the first part of the question. For the second part observe 
    $$
    \sigma_0^2 = \begin{bmatrix}
        1 & 0 \\ 0 & 1
    \end{bmatrix}\begin{bmatrix}
        1 & 0 \\ 0 & 1
    \end{bmatrix} = \begin{bmatrix}
        1 & 0 \\ 0 & 1
    \end{bmatrix} =I , \quad\quad \sigma_1^2 = \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix} \begin{bmatrix}
        0 & 1 \\ 1 & 0
    \end{bmatrix}= \begin{bmatrix}
        1 & 0 \\ 0 & 1
    \end{bmatrix} =I
    $$ $$
    \sigma_2^2= \begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix}\begin{bmatrix}
        0 & -i \\ i & 0
    \end{bmatrix} = \begin{bmatrix}
        -i^2 & 0 \\ 0 & -i^2
    \end{bmatrix} =I, \quad\quad \sigma_3^2=\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}\begin{bmatrix}
        1 & 0 \\ 0 & -1
    \end{bmatrix}=\begin{bmatrix}
        1 & 0 \\ 0 & 1
    \end{bmatrix} =I
    $$
    and that completes the second part.
    
\end{solution}





\subsection{Exercise} Verify that $$
AB = \frac{[A,B] + \{A,B\}}{2}.$$

\begin{solution}
    Expand out and add zero to find 
    \begin{equation*}
        \begin{split}
            AB &= AB/2 +AB/2 + (BA/2 -BA/2) \\
            &= (AB -BA)/2 + (AB + BA)/2 \\
            &= \frac{[A,B]}{2}+\frac{\{A,B\}}{2}=\frac{[A,B] + \{A,B\}}{2}
        \end{split}
    \end{equation*}
    
\end{solution}





\subsection{Exercise} Show that for $j,k=1,2,3$,
$$\sigma_j\sigma_k = \delta_{jk}I+i\sum\varepsilon_{jkl}\sigma_l.$$

\begin{solution}
    From 2.42 expand $\sigma_j\sigma_k$ to the form with the commutator and anti-commutator then apply 2.41 to the anti-commutator and the 2.40 to the commutator to find 
    $$\sigma_j\sigma_k =\frac{[\sigma_j,\sigma_k ]+\{\sigma_j,\sigma_k \}}{2}=\frac{[\sigma_j,\sigma_k ]}{2}+\frac{\{\sigma_j,\sigma_k \}}{2}=\frac{2i\sum\varepsilon_{jkl}\sigma_l}{2}+\frac{2\delta_{jk}I}{2}=\delta_{jk}I+i\sum\varepsilon_{jkl}\sigma_l$$ 
    which was to be shown. $\square$

    While not shown explicitly the sums are over $l=1,2,3$.
\end{solution}



\subsection{Exercise} Suppose $[A,B] = 0$, $\{A,B\}=0$, and $A$ is invertible. Show that $B$ must be 0. 

\begin{solution}
    If $[A,B] = 0$ and $\{A,B\}=0$ then $[A,B] = \{A,B\}=0$ so 
    $$AB-BA=AB +BA \implies \quad AB-BA-AB -BA=0 \implies \quad -2BA =0 $$
    Now divide out the $-2$ leaving $BA=0$ and note that since $A$ is invertible $AA^{-1}=I$ and if $A=0$ then $AA^{-1}\neq I$ thus $A\neq 0$. Therefore for $BA$ to equal $0$, $B$ must equal $0$. 
\end{solution}




\subsection{Exercise} Show that $[A,B]^\dagger=[B^\dagger,A^\dagger]$.

\begin{solution}
Recall that $$(AB)^\dagger=\bigg(\sum_{i,j}a_{ij}b_{ji}\bigg)^\dagger =\sum_{i,j}a_{ji}^*b_{ij}^* = B^\dagger A^\dagger$$
 using this it follows that
    $$[A,B]^\dagger=(AB-BA)^\dagger=((AB)^\dagger -(BA)^\dagger) = B^\dagger A^\dagger-A^\dagger B^\dagger=[B^\dagger,A^\dagger]$$
    which was to be shown.
\end{solution}




\subsection{Exercise} Show that $[A,B]=-[B,A]$.

\begin{solution}
    $$[A,B] = (AB-BA)=-(-AB+BA)=-(BA-AB)=-[B,A]$$
    which was to be shown.
\end{solution}




\subsection{Exercise} Suppose $A$ and $B$ are Hermitian. Show that $i[A,B]$ is Hermitian.

\begin{solution}
    Recall for a Hermitian matrix $M$ then $M=M^\dagger$ and observe 
    $$ 
    i[A,B]=i(AB-BA)=iAB-iBA=iA^\dagger B^\dagger-iB^\dagger A^\dagger=(-iBA)^\dagger +(iAB)^\dagger=(iAB-iBA)^\dagger=(i[A,B])^\dagger$$
    which was to be shown.
\end{solution}



\subsection{Exercise} What is the polar decomposition of a positive matrix $P$? Of a unitary matrix $U$? Of a Hermitian matrix, $H$?

\begin{solution}

    The polar decomposition of a positive matrix $P$ is simply $P=IP=PI$ where $P=J$ in 2.79 and $I=U$.

    The polar decomposition of a unitary matrix $U$ is $U=UI=IU$ since $U$ is already unitary and $I$ is positive.

    The polar decomposition of a Hermitian matrix $H$ can be split into two cases; $H$ is positive or all zero then refer to the above $P$ matrix decomposition, $H$ is negative definite. If $H$ is negative definite then $J=\sqrt{H^\dagger H}=\sqrt{H^2}=|H|$ is a positive operator (principal square root) and $U=sign(H)=\sum_i sign(\lambda_i)\ket{i}\bra{i}$. Thus $H=sign(H) \cdot |H|$ or in other words its decomposed into a positive matrix component and a negative unitary component. 
\end{solution}


\subsection{Exercise} Express the polar decomposition of a normal matrix in the outer product representation. 

\begin{solution}
    Recall a normal matrix is defined as $A^\dagger A=AA^\dagger$ and from \textit{Theorem 2.3} $A=UJ=KU$ where $J=\sqrt{A^\dagger A}$ and $K=\sqrt{AA^\dagger}$ then since $A$ is normal $J=K$ which are positive operators. Since the polar factors $U,J$ commute and are diagonalizable they share an eigenbasis. Using the spectral theorem to decompose them yields. 
    $$A = UJ= U\sum_i\lambda_i\ket{i}\bra{i}=\sum_i\lambda_i\ket{i}\bra{i}U \quad \text{ where $\lambda_i\geq0$}$$
    and $U=\sum_i\ket{e_i}\bra{i}=\sum_iu_i\ket{i}\bra{i}$. Plugging in gives
    $$
    A=U\sum_i|\lambda_i|\ket{i}\bra{i}=\sum_i|\lambda_i|\ket{e_i}\braket{i}\bra{i}=\sum_i|\lambda_i|\ket{e_i}\bra{i}=\sum_i u_i|\lambda_i|\ket{i}\bra{i}$$
    which is a polar decomposition of a normal matrix in outer product representation.
\end{solution}





\subsection{Exercise} Find the left and right polar decompositions of the matrix $$\begin{bmatrix}
    1 & 0 \\ 1 & 1
\end{bmatrix}$$

\begin{solution}
    Let $A$ equal the matrix above then $A^\dagger=\begin{bmatrix}
        1 & 1 \\ 0 & 1
    \end{bmatrix}$ then $J=\sqrt{A^\dagger A}=\sqrt{\begin{bmatrix}
        2 & 1 \\ 1 & 1
    \end{bmatrix}}$. To square root the matrix, start by diagonalizing so 
    $$\det\bigg(\begin{bmatrix}
        2-\lambda & 1 \\ 1 & 1-\lambda
    \end{bmatrix}\bigg)=\lambda^2-3\lambda+1=0\implies \quad\lambda_1,\lambda_2=\frac{3\pm\sqrt{5}}{2}$$
    and the eigenvectors are given by 
    $$
    \begin{bmatrix}
        2-\frac{3+\sqrt{5}}{2} & 1 \\ 1 & 1-\frac{3+\sqrt{5}}{2}
    \end{bmatrix}\begin{bmatrix}
        e_1 \\ e_2
    \end{bmatrix}=\textbf{0}\implies \vec{e_1}=\begin{bmatrix}
        \frac{1+\sqrt{5}}{2} \\ 1
    \end{bmatrix}, \quad \begin{bmatrix}
        2-\frac{3-\sqrt{5}}{2} & 1 \\ 1 & 1-\frac{3+\sqrt{5}}{2}
    \end{bmatrix}\begin{bmatrix}
        e_1 \\ e_2
    \end{bmatrix}=\textbf{0}\implies \vec{e_1}=\begin{bmatrix}
        \frac{1-\sqrt{5}}{2} \\ 1
    \end{bmatrix}$$
    then $J=\sqrt{\lambda_1}\ket{e_1}\bra{e_1}/\norm{e_1}^2+\sqrt{\lambda_2}\ket{e_2}\bra{e_2}/\norm{e_2}^2$ $$=\sqrt{\frac{3+\sqrt{5}}{2}}\begin{bmatrix}
        \frac{(1+\sqrt{5} )^2}{4} & \frac{1+\sqrt{5}}{2} \\ \frac{1+\sqrt{5}}{2} & 1
    \end{bmatrix}/(10 +2\sqrt{5}) + 
    \sqrt{\frac{3-\sqrt{5}}{2}}\begin{bmatrix}
        \frac{(1-\sqrt{5} )^2}{4} & \frac{1-\sqrt{5}}{2} \\ \frac{1-\sqrt{5}}{2} & 1
    \end{bmatrix}/(10 +2\sqrt{5})= \frac{1}{\sqrt{5}}\begin{bmatrix}
        3 & 1 \\ 1 & 2
    \end{bmatrix}
    $$ 
    and $K=J^{-1}=\frac{1}{5}\begin{bmatrix}
        2\sqrt{5} & -\sqrt{5} \\ -\sqrt{5} & 3 \sqrt{5}
    \end{bmatrix}$. Now all that is left is to calculate $U$ using $A=UJ\implies U=AJ^{-1}=AK$
    $$AK=\begin{bmatrix}
        1 & 0 \\ 1 & 1
    \end{bmatrix}\frac{1}{5}\begin{bmatrix}
        2\sqrt{5} & -\sqrt{5} \\ -\sqrt{5} & 3 \sqrt{5}
    \end{bmatrix}=\frac{1}{\sqrt{5}}\begin{bmatrix}
        2 & -1 \\ 1 & 2
    \end{bmatrix}=U$$
    where $UJ$ or $KU$ represent the left and right polar representations respectively.
\end{solution}



\subsection{Exercise} Verify that the Hadamard gate $H$ is unitary.

\begin{solution} 
    A unitary $U$ is defined as $U^\dagger U = UU^\dagger=I$ and 
    $H=\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 & 1 \\ 1 & -1
    \end{bmatrix}$ so note that $H^\dagger=(H^*)^T=H$ since $H$ is real and symmetric then 
    $$HH=\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 & 1 \\ 1 & -1
    \end{bmatrix}\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 & 1 \\ 1 & -1
    \end{bmatrix}=\frac{1}{2}\begin{bmatrix}
        1 +1 & 1- 1 \\ 1 -1 & 1+1
    \end{bmatrix}=\frac{1}{2}\begin{bmatrix}
        2 & 0 \\ 0 & 2
    \end{bmatrix}=I$$
    thus $H$ is unitary.
\end{solution}


\subsection{Exercise} Verify that $H^2=I$

\begin{solution}
    Similar to 2.51 calculate 
    $$H^2=\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 & 1 \\ 1 & -1
    \end{bmatrix}\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 & 1 \\ 1 & -1
    \end{bmatrix}=\frac{1}{2}\begin{bmatrix}
        1 +1 & 1- 1 \\ 1 -1 & 1+1
    \end{bmatrix}=\frac{1}{2}\begin{bmatrix}
        2 & 0 \\ 0 & 2
    \end{bmatrix}=I \quad \text{verified.}$$
\end{solution}




\subsection{Exercise} What are the eigenvalues and eigenvectors of $H$?

\begin{solution}
    Calculating
    $$\det(H-\lambda I)=
    \begin{bmatrix}
        \frac{1}{\sqrt{2}}-\lambda & \frac{1}{\sqrt{2}} \\
        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}-\lambda
    \end{bmatrix}=
    \lambda^2 -1\quad\implies\quad \lambda_{1,2}=\pm 1$$
    with eigenvectors 
    $$\begin{bmatrix}
        \frac{1}{\sqrt{2}}-1 & \frac{1}{\sqrt{2}} \\
        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}-1
    \end{bmatrix}\ket{e_1}=\textbf{0}\implies\ket{e_1}=\begin{bmatrix}
        1+\sqrt{2} \\ 1
    \end{bmatrix}, \quad
    \begin{bmatrix}
        \frac{1}{\sqrt{2}}+1 & \frac{1}{\sqrt{2}} \\
        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}+1
    \end{bmatrix}\ket{e_2}=\textbf{0}\implies\ket{e_2}=\begin{bmatrix}
        1-\sqrt{2} \\ 1
    \end{bmatrix}$$
    with normalization of 
    $$\frac{\ket{e_1}}{\sqrt{\braket{e_1}}}=\frac{1}{\sqrt{4+2\sqrt{2}}}\begin{bmatrix}
        1+\sqrt{2} \\ 1
    \end{bmatrix}, \quad \frac{\ket{e_2}}{\sqrt{\braket{e_2}}}=\frac{1}{\sqrt{4-2\sqrt{2}}}\begin{bmatrix}
        1-\sqrt{2} \\ 1
    \end{bmatrix}$$
\end{solution}



\subsection{Exercise} Suppose $A$ and $B$ are commuting Hermitian operators. Prove that $\exp(A)\exp(B)=\exp(A+B)$. (\textit{Hint}: Use the results of Section 2.1.9.)

\begin{solution}
    Recall that 2.1.9 discusses the commutator and anti-commutator but more importantly also contains \textit{Theorem 2.2} which states that $[A,B]=0 \iff $ there exists an orthonormal basis s.t. both $A$ and $B$ are diagonal w.r.t. that basis.

    Since $A$ and $B$ commute then $[A,B]=0$ so they simultaneously diagonalizable.
    $$\exp(A)\exp(B)=\sum_i\exp(a_i)\ket{i}\bra{i}\sum_i\exp(b_i)\ket{i}\bra{i}
    =\sum_i\exp(a_i)\exp(b_i)\ket{i}\bra{i}=\sum_i\exp(a_i+b_i)\ket{i}\bra{i}
    $$ 
    $\quad 
    =\exp(A+B)\quad \square$
    which was to be shown.
\end{solution}



\subsection{Exercise} Prove that $U(t_1,t_2)$ defined in Equation (2.91) is unitary.

\begin{solution}
    Recall that a unitary $U^\dagger U=UU^\dagger=I$ and  Equation 2.91 was defined as
    $$ U(t_1,t_2)=\exp\bigg[\frac{-iH(t_2-t_1)}{\hbar}\bigg] $$
    where Hamiltonian $H$ is a hermitian operator with spectral decomposition $H=\sum E\ket{E}\bra{E}$ so
    $$
    U(t_1,t_2)=\exp\bigg[\frac{-iH(t_2-t_1)}{\hbar}\bigg]
    =\exp\bigg[\frac{-i\sum_EE\ket{E}\bra{E}(t_2-t_1)}{\hbar}\bigg] 
    = \sum_E\exp\bigg[\frac{-iE(t_2-t_1)}{\hbar}\bigg] \ket{E}\bra{E}
    $$ and note $e^{A^\dagger}=(e^A)^\dagger$ so its adjoint is
    $$
    U(t_1,t_2)^\dagger=\exp\bigg[\frac{-iH(t_2-t_1)}{\hbar}\bigg]^\dagger
    =\exp\bigg[\frac{-i\sum_EE\ket{E}\bra{E}(t_2-t_1)}{\hbar}\bigg] ^{T*}
    = \sum_E\exp\bigg[\frac{iE(t_2-t_1)}{\hbar}\bigg] \ket{E}\bra{E}
    $$
    then 
    \begin{equation*}
        \begin{split}
            UU^\dagger&=\sum_E\exp\bigg[\frac{-iE(t_2-t_1)}{\hbar}\bigg] \ket{E}\bra{E}\sum_{F}\exp\bigg[\frac{iF(t_2-t_1)}{\hbar}\bigg] \ket{F}\bra{F} \\
            &=\sum_E\exp\bigg[\frac{-iE(t_2-t_1)}{\hbar}\bigg]\exp\bigg[\frac{iE(t_2-t_1)}{\hbar}\bigg] \delta_{EF}\ket{E}\bra{E} \\
            &=\sum_E\exp\bigg[\frac{-iE(t_2-t_1)}{\hbar}+\frac{iE(t_2-t_1)}{\hbar}\bigg] \ket{E}\bra{E} \\
            &=\sum_E e^0\ket{E}\bra{E} = I
        \end{split}
    \end{equation*}
    which was to be shown.
\end{solution}




\subsection{Exercise} Use the spectral decomposition to show that $K\equiv-i\log(U)$ is Hermitian for any unitary $U$, and thus $U=\exp (iK)$ for some hermitian $K$.

\begin{solution}
    Recall that a unitary $U^\dagger U=UU^\dagger=I$ with eigenvalues of form $e^{i\theta}$ for a real $\theta$ and Hermitian $A^\dagger=A$ "self-adjoint" so to show $K\equiv-i\log(U)$ expand $U$ into its spectral decomposition with eigenvalue $\lambda_u=e^{i\theta_u}$
    $$K=-i\log(U)
    =-i\log\bigg(\sum_u e^{i\theta_u}\ket{u}\bra{u}\bigg)
    =-i\sum_u \log (e^{i\theta_u})\ket{u}\bra{u}
    =-i\sum_u i\theta_u\ket{u}\bra{u}
    =\sum_u \theta_u\ket{u}\bra{u}$$
    which is a real diagonal matrix in basis $u$ so $K=K^\dagger$ thus it is Hermitian. 
\end{solution}
















\end{document}
